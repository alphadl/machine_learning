{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Latent linear models (Dimension reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis\n",
    "\n",
    "Purpose: group features that are highly correlated to do dimension reduction, these \"groups\" of features are hidden variables\n",
    "\n",
    "Real vector latent variables: $z_i \\in \\mathbb{R}^L$: $$p \\left( \\mathbf { z } _ { i } \\right) = \\mathcal { N } \\left( \\mathbf { z } _ { i } | \\boldsymbol { \\mu } _ { 0 } , \\mathbf { \\Sigma } _ { 0 } \\right)$$\n",
    "\n",
    "Likelihood for original data space $x_i \\in \\mathbb{R}^D$:\n",
    "$$p \\left( \\mathbf { x } _ { i } | \\mathbf { z } _ { i } , \\boldsymbol { \\theta } \\right) = \\mathcal { N } \\left( \\mathbf { W } \\mathbf { z } _ { i } + \\boldsymbol { \\mu } , \\mathbf { \\Psi } \\right)$$\n",
    "\n",
    "where $W \\in \\mathbb{R}^{D \\times L}$ is factor loading matrix, $\\Psi \\in \\mathbb{R}^{D \\times D}$ is covariance matrix, take $\\Psi$ to be diagonal matrix ==> factor analysis (FA). \n",
    "\n",
    "When $\\mathbf { \\Psi } = \\sigma ^ { 2 } \\mathbf { I }$ ==> probabilistic principal components analysis (PPCA). \n",
    "When $\\sigma \\rightarrow 0$, we have principal components analysis (PCA)\n",
    "\n",
    "Marginal distribution: $$\\begin{aligned} p \\left( \\mathbf { x } _ { i } | \\boldsymbol { \\theta } \\right) & = \\int \\mathcal { N } \\left( \\mathbf { x } _ { i } | \\mathbf { W } \\mathbf { z } _ { i } + \\boldsymbol { \\mu } , \\mathbf { \\Psi } \\right) \\mathcal { N } \\left( \\mathbf { z } _ { i } | \\boldsymbol { \\mu } _ { 0 } , \\mathbf { \\Sigma } _ { 0 } \\right) d \\mathbf { z } _ { i } \\\\ & = \\mathcal { N } \\left( \\mathbf { x } _ { i } | \\mathbf { W } \\boldsymbol { \\mu } _ { 0 } + \\boldsymbol { \\mu } , \\mathbf { \\Psi } + \\mathbf { W } \\boldsymbol { \\Sigma } _ { 0 } \\mathbf { W } ^ { T } \\right) \\end{aligned}$$\n",
    "\n",
    "Low-rank decomposition: $$\\mathbf { C } \\triangleq \\operatorname { cov } [ \\mathbf { x } ] = \\mathbf { W } \\mathbf { W } ^ { T } + \\mathbf { \\Psi }$$\n",
    "\n",
    "+ EM for factor analysis:\n",
    "Expected log likelihood:\n",
    "$$Q = E \\left[ \\log \\prod _ { i } ( 2 \\pi ) ^ { p / 2 } | \\Psi | ^ { - 1 / 2 } \\exp \\left\\{ - \\frac { 1 } { 2 } \\left[ \\mathrm { x } _ { i } - W \\mathrm { z } \\right] ^ { \\prime } \\Psi ^ { - 1 } \\left[ \\mathrm { x } _ { i } - W \\mathrm { z } \\right] \\right\\} \\right]$$\n",
    "\n",
    "solve that we obtain:\n",
    "$$\\begin{aligned} W ^ { \\mathrm { new } } & = \\left( \\sum _ { i = 1 } ^ { n } \\mathbf { x } _ { i } E ( \\mathbf { z } | \\mathbf { x } _ { i } ) ^ { \\prime } \\right) \\left( \\sum _ { l = 1 } ^ { n } E \\left( \\mathbf { z z } ^ { \\prime } | \\mathbf { x } _ { l } \\right) \\right) ^ { - 1 } \\\\ \\Psi ^ { \\mathrm { new } } & = \\frac { 1 } { n } \\operatorname { diag } \\left\\{ \\sum _ { i = 1 } ^ { n } \\mathbf { x } _ { \\mathbf { i } } \\mathbf { x } _ { i } ^ { \\prime } - W ^ { \\mathrm { new } } E [ \\mathbf { z } | \\mathbf { x } _ { i } ] \\mathbf { x } _ { i } ^ { \\prime } \\right\\} \\end{aligned}$$\n",
    "\n",
    "### Inference of latent factors:\n",
    "\n",
    "$$\\begin{aligned} p \\left( \\mathbf { z } _ { i } | \\mathbf { x } _ { i } , \\boldsymbol { \\theta } \\right) & = \\mathcal { N } \\left( \\mathbf { z } _ { i } | \\mathbf { m } _ { i } , \\mathbf { \\Sigma } _ { i } \\right) \\\\ \\mathbf { \\Sigma } _ { i } \\triangleq & \\left( \\mathbf { \\Sigma } _ { 0 } ^ { - 1 } + \\mathbf { W } ^ { T } \\mathbf { \\Psi } ^ { - 1 } \\mathbf { W } \\right) ^ { - 1 } \\\\ \\mathbf { m } _ { i } \\triangleq & \\boldsymbol { \\Sigma } _ { i } \\left( \\mathbf { W } ^ { T } \\mathbf { \\Psi } ^ { - 1 } \\left( \\mathbf { x } _ { i } - \\boldsymbol { \\mu } \\right) + \\mathbf { \\Sigma } _ { 0 } ^ { - 1 } \\boldsymbol { \\mu } _ { 0 } \\right) \\end{aligned}$$\n",
    "\n",
    "\n",
    "### Mixtures of factor analysers (MFA):\n",
    "$q_i \\in \\{1, \\ldots,  K\\}$ specify which subspace we shuld use to generate data\n",
    "$$\\begin{aligned} p \\left( \\mathbf { x } _ { i } | \\mathbf { z } _ { i } , q _ { i } = k , \\boldsymbol { \\theta } \\right) & = \\mathcal { N } \\left( \\mathbf { x } _ { i } | \\boldsymbol { \\mu } _ { k } + \\mathbf { W } _ { k } \\mathbf { z } _ { i } , \\mathbf { \\Psi } \\right) \\\\ p \\left( \\mathbf { z } _ { i } | \\boldsymbol { \\theta } \\right) & = \\mathcal { N } \\left( \\mathbf { z } _ { i } | \\mathbf { 0 } , \\mathbf { I } \\right) \\\\ p \\left( q _ { i } | \\boldsymbol { \\theta } \\right) & = \\operatorname { Cat } \\left( q _ { i } | \\boldsymbol { \\pi } \\right) \\end{aligned}$$\n",
    "\n",
    "![](../images/12.MFA.png)\n",
    "\n",
    "+ EM for mixtures of factor analysis models:\n",
    "Expected log likelihood:\n",
    "\n",
    "$$Q = E \\left[ \\log \\prod _ { i } \\prod _ { j } \\left\\{ ( 2 \\pi ) ^ { p / 2 } | \\Psi | ^ { - 1 / 2 } \\exp \\left\\{ - \\frac { 1 } { 2 } \\left[ \\mathrm { x } _ { i } - \\boldsymbol { \\mu } _ { j } - W _ { j } \\mathrm { z } \\right] ^ { \\prime } \\Psi ^ { - 1 } \\left[ \\mathrm { x } _ { i } - \\boldsymbol { \\mu } _ { j } - W _ { j } \\mathrm { z } \\right] \\right\\} \\right\\} ^ { w _ { j } } \\right]$$\n",
    "\n",
    "Let $r _ { i c } \\triangleq p \\left( q _ { i } = c | \\mathbf { x } _ { i } , \\boldsymbol { \\theta } \\right) \\propto \\pi _ { c } \\mathcal { N } \\left( \\mathbf { x } _ { i } | \\boldsymbol { \\mu } _ { c } , \\mathbf { W } _ { c } \\mathbf { W } _ { c } ^ { T } + \\mathbf { \\Psi } \\right)$ be the responsibility of cluster $c$ for dataa point $i$\n",
    "\n",
    "using MLE: we solve for:\n",
    "$$\\begin{aligned} \\hat { \\mathbf { \\mathbf { w } } } _ { c } & = \\left[ \\sum _ { i } r _ { i c } \\mathbf { x } _ { i } \\mathbf { b } _ { i c } ^ { T } \\right] \\left[ \\sum _ { i } r _ { i c } \\mathbf { C } _ { i c } \\right] ^ { - 1 } \\\\ \\hat { \\mathbf { \\Psi } } & = \\frac { 1 } { N } \\operatorname { diag } \\left\\{ \\sum _ { i c } r _ { i c } \\left( \\mathbf { x } _ { i } - \\hat { \\mathbf { w } } _ { c } \\mathbf { b } _ { i c } \\right) \\mathbf { x } _ { i } ^ { T } \\right\\} \\\\ \\hat { \\pi } _ { c } & = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } r _ { i c } \\end{aligned}$$\n",
    "\n",
    "where: $\\tilde{W}_c = (W_c, \\mu_c), \\tilde{z} = (z, 1)$ and:\n",
    "$$\\mathbf { b } _ { i c } \\triangleq \\mathbb { E } [ \\tilde { \\mathbf { z } } | \\mathbf { x } _ { i } , q _ { i } = c ] = \\left[ \\mathbf { m } _ { i c } ; 1 \\right]$$\n",
    "\n",
    "$$\\mathbf { C } _ { i c } \\triangleq \\mathbb { E } \\left[ \\tilde { \\mathbf { z } } \\tilde { \\mathbf { z } } ^ { T } | \\mathbf { x } _ { i } , q _ { i } = c \\right] = \\left( \\begin{array} { c } { \\mathbb { E } \\left[ \\mathbf { z } ^ { T } | \\mathbf { x } _ { i } , q _ { i } = c \\right] } & \\mathbb { E } [ \\mathbf { z } | \\mathbf { x } _ { i } , q _ { i } = c ]  \\\\ { \\mathbb { E } [ \\mathbf { z } | \\mathbf { x } _ { i } , q _ { i } = c ] ^ { T } } & { 1 } \\end{array} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "Theorem: Suppose we want to find an orthogonal set of $L$ linear basis vectors $w_j \\in \\mathbb{R}^D$, and the corresponding scores $z_i \\in \\mathbb{R}^L$, such that we minimize the average reconstruction error:\n",
    "\n",
    "$$J ( \\mathbf { W } , \\mathbf { Z } ) = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\left\\| \\mathbf { x } _ { i } - \\hat { \\mathbf { x } } _ { i } \\right\\| ^ { 2 } = \\left\\| \\mathbf { X } - \\mathbf { W } \\mathbf { Z } ^ { T } \\right\\| _ { F } ^ { 2 }$$\n",
    "\n",
    "where $Z$ is an $N \\times L$ matrix with the $z_i$ in its rows, and $||A||_F$ is the Frobenius norm of matrix $A$ defined by:\n",
    "\n",
    "$$\\| \\mathbf { A } \\| _ { F } = \\sqrt { \\sum _ { i = 1 } ^ { m } \\sum _ { j = 1 } ^ { n } a _ { i j } ^ { 2 } } = \\sqrt { \\operatorname { tr } \\left( \\mathbf { A } ^ { T } \\mathbf { A } \\right) } = \\| \\mathbf { A } ( : ) \\| _ { 2 }$$\n",
    "\n",
    "The optimal solution is obtained by setting $\\hat { \\mathbf { W } } = \\mathbf { V } _ { L }$, where $V_L$ contains the $L$ eigenvectors with largest eigenvalues of the emperical covariance matrix $\\hat { \\mathbf { \\Sigma } } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\mathbf { x } _ { i } \\mathbf { x } _ { i } ^ { T }$. \n",
    "\n",
    "The optimal low-dimensional encoding of the data is given by $\\hat { \\mathbf { z } } _ { i } = \\mathbf {\\hat{ W }} ^ { T } \\mathbf { x } _ { i }$, which is an orthogonal projection of the data onto the column space spanned by the eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic PCA (PPCA)\n",
    "\n",
    "Theorem: Consider a factor analysis model in which $\\Psi = \\sigma^2I$ and $W$ is orthogonal. The observed data log likelihood is given by:\n",
    "\n",
    "$$\\log p ( \\mathbf { X } | \\mathbf { W } , \\sigma ^ { 2 } ) = - \\frac { N } { 2 } \\ln | \\mathbf { C } | - \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { N } \\mathbf { x } _ { i } ^ { T } \\mathbf { C } ^ { - 1 } \\mathbf { x } _ { i } = - \\frac { N } { 2 } \\ln | \\mathbf { C } | + \\operatorname { tr } \\left( \\mathbf { C } ^ { - 1 } \\hat { \\mathbf { \\Sigma } } \\right)$$\n",
    "\n",
    "where: $\\mathbf { C } = \\mathbf { W } \\mathbf { W } ^ { T } + \\sigma ^ { 2 } \\mathbf { I }$ and $\\mathbf { S } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\mathbf { x } _ { i } \\mathbf { x } _ { i } ^ { T } = ( 1 / N ) \\mathbf { X } ^ { T } \\mathbf { X }$\n",
    "\n",
    "We can use SVD (Eigen decomposition) for solving that, but we can use EM algorithm to the expected data log likelihood as follows:\n",
    "\n",
    "$$\\begin{array} { l } { \\bullet \\mathbf { E } \\text { step } \\tilde { \\mathbf { Z } } = \\left( \\mathbf { W } ^ { T } \\mathbf { W } \\right) ^ { - 1 } \\mathbf { W } ^ { T } \\tilde { \\mathbf { X } } } \\\\ { \\bullet \\mathbf { M } \\text { step } \\mathbf { W } = \\tilde { \\mathbf { X } } \\tilde { \\mathbf { Z } } ^ { T } \\left( \\tilde { \\mathbf { Z } } \\tilde { \\mathbf { Z } } ^ { T } \\right) ^ { - 1 } } \\end{array}$$\n",
    "\n",
    "Advantage of using EM:\n",
    "+ EM can be faster\n",
    "+ EM can be implemented in an online fashion\n",
    "+ EM can handle missing data in a simple way\n",
    "+ EM can be extended to handle mixtures of PPCA/FA models\n",
    "+ EM can be modified to variational EM or to variational Bayes EM to fit more complex models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
