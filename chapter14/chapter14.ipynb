{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let $x_i \\in \\mathbb{R}^D$, we have some way of measuring the similarity between objects, that does not require preprocessing them into feature vector format. Let $\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) \\geq 0$ be some measure of similarity between objects $x, x' \\in X$, we call $\\kappa$ a kernel function.\n",
    "\n",
    "Kernel function: $\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) \\in \\mathbb { R }$ this function is: symmetric $\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) = \\kappa \\left( \\mathbf { x } ^ { \\prime } , \\mathbf { x } \\right)$ and non-negative: $\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) \\geq 0$\n",
    "\n",
    "### RBF kernels (Gaussian Kernels)\n",
    "\n",
    "$$\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) = \\exp \\left( - \\frac { 1 } { 2 } \\left( \\mathbf { x } - \\mathbf { x } ^ { \\prime } \\right) ^ { T } \\mathbf { \\Sigma } ^ { - 1 } \\left( \\mathbf { x } - \\mathbf { x } ^ { \\prime } \\right) \\right)$$\n",
    "\n",
    "$$\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) = \\exp \\left( - \\frac { 1 } { 2 } \\sum _ { j = 1 } ^ { D } \\frac { 1 } { \\sigma _ { j } ^ { 2 } } \\left( x _ { j } - x _ { j } ^ { \\prime } \\right) ^ { 2 } \\right)$$\n",
    "\n",
    "If $\\Sigma$ is spherical: we have isotropic kernels:\n",
    "$$\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) = \\exp \\left( - \\frac { \\left\\| \\mathbf { x } - \\mathbf { x } ^ { \\prime } \\right\\| ^ { 2 } } { 2 \\sigma ^ { 2 } } \\right)$$\n",
    "\n",
    "$\\sigma^2$ is bandwidth, Radial basis function or RBF kernel\n",
    "\n",
    "### Kernel for comparing documents:\n",
    "+ Cosine similarity:\n",
    "$$\\kappa \\left( \\mathbf { x } _ { i } , \\mathbf { x } _ { i ^ { \\prime } } \\right) = \\frac { \\mathbf { x } _ { i } ^ { T } \\mathbf { x } _ { i ^ { \\prime } } } { \\left\\| \\mathbf { x } _ { i } \\right\\| _ { 2 } \\left\\| \\mathbf { x } _ { i ^ { \\prime } } | | _ { 2 } \\right. }$$\n",
    "\n",
    "+ TF-IDF Kernel: TF-IDF = Term frequency inverse document frequency\n",
    "$$\\operatorname { tf } \\left( x _ { i j } \\right) \\triangleq \\log \\left( 1 + x _ { i j } \\right)$$\n",
    "\n",
    "$$\\operatorname { idf } ( j ) \\triangleq \\log \\frac { N } { 1 + \\sum _ { i = 1 } ^ { N } \\mathbb { I } \\left( x _ { i j } > 0 \\right) }$$\n",
    "\n",
    "where $N$ is the total number of documents, the denominator counts how many documents contain term $j$:\n",
    "\n",
    "$$\\mathrm { tf } - \\mathrm { idf } \\left( \\mathbf { x } _ { i } \\right) \\triangleq \\left[ \\mathrm { tf } \\left( x _ { i j } \\right) \\times \\mathrm { idf } ( j ) \\right] _ { j = 1 } ^ { V }$$\n",
    "\n",
    "$$\\kappa \\left( \\mathbf { x } _ { i } , \\mathbf { x } _ { i ^ { \\prime } } \\right) = \\frac { \\phi \\left( \\mathbf { x } _ { i } \\right) ^ { T } \\phi \\left( \\mathbf { x } _ { i ^ { \\prime } } \\right) } { \\left\\| \\phi \\left( \\mathbf { x } _ { i } \\right) \\right\\| _ { 2 } \\left\\| \\phi \\left( \\mathbf { x } _ { i ^ { \\prime } } \\right) \\right\\| _ { 2 } }$$\n",
    "\n",
    "where $\\phi ( \\mathbf { x } ) = tf-idf ( \\mathbf { x } )$\n",
    "\n",
    "### Mercer (positive definite) kernels:\n",
    "Gram matrix:\n",
    "$$\\mathbf { K } = \\left( \\begin{array} { c c c } { \\kappa \\left( \\mathbf { x } _ { 1 } , \\mathbf { x } _ { 1 } \\right) } & { \\cdots } & { \\kappa \\left( \\mathbf { x } _ { 1 } , \\mathbf { x } _ { N } \\right) } \\\\ { } & { \\vdots } & { } \\\\ { \\kappa \\left( \\mathbf { x } _ { N } , \\mathbf { x } _ { 1 } \\right) } & { \\cdots } & { \\kappa \\left( \\mathbf { x } _ { N } , \\mathbf { x } _ { N } \\right) } \\end{array} \\right)$$\n",
    "\n",
    "If the kernel is Mercer, then there exists a function $\\phi$ mapping $x \\in X$ to $\\mathbb{R}^D$ such that: \n",
    "$$\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) = \\phi ( \\mathbf { x } ) ^ { T } \\phi \\left( \\mathbf { x } ^ { \\prime } \\right)$$\n",
    "\n",
    "where $\\phi$ depends on the eigen functions of $\\kappa$\n",
    "\n",
    "Examples:\n",
    "+ Polynomial kernel: $\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) = \\left( \\gamma \\mathbf { x } ^ { T } \\mathbf { x } ^ { \\prime } + r \\right) ^ { M }$ where $r > 0$, $M$ is degree.\n",
    "+ Sigmoid kernel: $\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) = \\tanh \\left( \\gamma \\mathbf { x } ^ { T } \\mathbf { x } ^ { \\prime } + r \\right)$\n",
    "\n",
    "### Linear Kernels:\n",
    "$$\\kappa \\left( \\mathbf { x } , \\mathbf { x } ^ { \\prime } \\right) = \\mathbf { x } ^ { T } \\mathbf { x } ^ { \\prime }$$\n",
    "\n",
    "It is useful if the original data is already high dimensiona and original features are individually informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Kernels\n",
    "\n",
    "### Kernel machines:\n",
    "$$\\phi ( \\mathbf { x } ) = \\left[ \\kappa \\left( \\mathbf { x } , \\boldsymbol { \\mu } _ { 1 } \\right) , \\ldots , \\kappa \\left( \\mathbf { x } , \\boldsymbol { \\mu } _ { K } \\right) \\right]$$\n",
    "\n",
    "where $\\boldsymbol { \\mu } _ { k } \\in \\mathcal { X }$ are a set of $K$ centroids (learnable parameters): if $\\kappa$ is an RBF kernel, this is called RBF network. ==> Kernelized feature vector\n",
    "\n",
    "+ Logistic regression: $p ( y | \\mathbf { x } , \\boldsymbol { \\theta } ) = \\operatorname { Ber } \\left( \\mathbf { w } ^ { T } \\boldsymbol { \\phi } ( \\mathbf { x } ) \\right)$ we will have non-linear decision boundary.\n",
    "+ Linear regression: $p ( y | \\mathbf { x } , \\boldsymbol { \\theta } ) = \\mathcal { N } \\left( \\mathbf { w } ^ { T } \\boldsymbol { \\phi } ( \\mathbf { x } ) , \\sigma ^ { 2 } \\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machines (SVMs)\n",
    "\n",
    "### SVMs for regression\n",
    "+ epsilon (margin) insensitive loss function: \n",
    "$$L _ { \\epsilon } ( y , \\hat { y } ) \\triangleq \\left\\{ \\begin{array} { c c } { 0 } & { \\text { if } | y - \\hat { y } | < \\epsilon } \\\\ { | y - \\hat { y } | - \\epsilon } & { \\text { otherwise } } \\end{array} \\right.$$\n",
    "\n",
    "+ Objective function:\n",
    "$$J = C \\sum _ { i = 1 } ^ { N } L _ { \\epsilon } \\left( y _ { i } , \\hat { y } _ { i } \\right) + \\frac { 1 } { 2 } \\| \\mathbf { w } \\| ^ { 2 }$$\n",
    "\n",
    "+ Introduce slack variables $\\xi$: \n",
    "$$\\begin{aligned} y _ { i } & \\leq f \\left( \\mathbf { x } _ { i } \\right) + \\epsilon + \\xi _ { i } ^ { + } \\\\ y _ { i } & \\geq f \\left( \\mathbf { x } _ { i } \\right) - \\epsilon - \\xi _ { i } ^ { - } \\end{aligned}$$\n",
    "\n",
    "+ New objective function: \n",
    "$$J = C \\sum _ { i = 1 } ^ { N } \\left( \\xi _ { i } ^ { + } + \\xi _ { i } ^ { - } \\right) + \\frac { 1 } { 2 } \\| \\mathbf { w } \\| ^ { 2 }$$\n",
    "\n",
    "+ So we have $$\\hat { \\mathbf { w } } = \\sum _ { i } \\alpha _ { i } \\mathbf { x } _ { i }$$\n",
    "\n",
    "    where $\\alpha_i \\geq 0$ are Langrange multipliers and sparse. The $x_i$ for which $\\alpha_i > 0$ are called support vectors, these points lie on or outside the $\\epsilon$ tube\n",
    "\n",
    "+ make prediction: \n",
    "$$\\hat { y } ( \\mathbf { x } ) = \\hat { w } _ { 0 } + \\hat { \\mathbf { w } } ^ { T } \\mathbf { x }$$\n",
    "\n",
    "+ if we use kernel:\n",
    "$$\\hat { y } ( \\mathbf { x } ) = \\hat { w } _ { 0 } + \\sum _ { i } \\alpha _ { i } \\kappa \\left( \\mathbf { x } _ { i } , \\mathbf { x } \\right)$$\n",
    "\n",
    "### SVMs for classification\n",
    "+ Hinge loss: $$L _ { \\text { hinge } } ( y , \\eta ) = \\max ( 0,1 - y \\eta ) = ( 1 - y \\eta ) _ { + }$$\n",
    "where $\\eta = f ( \\mathbf { x } ) = \\mathbf { w } ^ { T } \\mathbf { x } + w _ { 0 }$ \n",
    "\n",
    "+ Objective function:\n",
    "$$\\min _ { \\mathbf { w } , w _ { 0 } } \\frac { 1 } { 2 } \\| \\mathbf { w } \\| ^ { 2 } + C \\sum _ { i = 1 } ^ { N } \\left( 1 - y _ { i } f \\left( \\mathbf { x } _ { i } \\right) \\right) _ { + }$$\n",
    "\n",
    "+ Introducing slack variable $\\xi$:\n",
    "$$\\min _ { \\mathbf { w } , w _ { 0 } , \\xi } \\frac { 1 } { 2 } \\| \\mathbf { w } \\| ^ { 2 } + C \\sum _ { i = 1 } ^ { N } \\xi _ { i } \\quad \\text { s.t. } \\quad \\xi _ { i } \\geq 0 , y _ { i } \\left( \\mathbf { x } _ { i } ^ { T } \\mathbf { w } + w _ { 0 } \\right) \\geq 1 - \\xi _ { i } , i = 1 : N$$\n",
    "\n",
    "+ So we have $$\\hat { \\mathbf { w } } = \\sum _ { i } \\alpha _ { i } \\mathbf { x } _ { i }$$\n",
    "where $\\alpha_i = \\lambda_i y_i$, $\\lambda_i$ are Langrange multipliers. The $x_i$ for which $\\alpha_i > 0$ are called support vectors, which are either incorrectly classified or are classified correctly but are on or inside the margin\n",
    "\n",
    "+ make prediction:\n",
    "$$\\hat { y } ( \\mathbf { x } ) = \\operatorname { sgn } ( f ( \\mathbf { x } ) ) = \\operatorname { sgn } \\left( \\hat { w } _ { 0 } + \\hat { \\mathbf { w } } ^ { T } \\mathbf { x } \\right)$$\n",
    "\n",
    "+ use kernel:\n",
    "$$\\hat { y } ( \\mathbf { x } ) = \\operatorname { sgn } \\left( \\hat { w } _ { 0 } + \\sum _ { i = 1 } ^ { N } \\alpha _ { i } \\kappa \\left( \\mathbf { x } _ { i } , \\mathbf { x } \\right) \\right)$$\n",
    "\n",
    "### Summary:\n",
    "We need three ingredients: \n",
    "+ The kernel trick: The kernel trick is necessary to prevent underÔ¨Åtting, i.e., to ensure that the feature vector is sufficiently rich that a linear classiÔ¨Åer can separate the data.\n",
    "+ The sparsity and large margin principles are necessary to prevent overÔ¨Åtting, i.e., to ensure that we do not use all the basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
