{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Generative Models for Discrete Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayesian Concept Learning\n",
    "+ concept learning = learning a meaning of a word\n",
    "+ induction\n",
    "+ hypothesis space\n",
    "+ version space: subset of hypothesis $H$ that is consistent with the data $D$ \n",
    "+ suspicious coincidences\n",
    "+ size principle: model favors the simplest (smallest) hypothesis consistent with the data = Occam's razor\n",
    "+ likelihood: \n",
    "$$p ( \\mathcal { D } | h ) = \\left[ \\frac { 1 } { \\operatorname { size } ( h ) } \\right] ^ { N } = \\left[ \\frac { 1 } { | h | } \\right] ^ { N }\n",
    "$$\n",
    "+ extension of a concept: is just the set of numbers that belong to it.\n",
    "+ prior is the mechanism by which background knowledge can be brought to bear on a problem. Without this, rapid learning (from small samples sizes) is impossible.\n",
    "+ posterior (internal belief state about the world): likelihood times the prior, normalized\n",
    "$$ p ( h | \\mathcal { D } ) = \\frac { p ( \\mathcal { D } | h ) p ( h ) } { \\sum _ { h ^ { \\prime } \\in \\mathcal { H } } p \\left( \\mathcal { D } , h ^ { \\prime } \\right) } = \\frac { p ( h ) \\mathbb { I } ( \\mathcal { D } \\in h ) / | h | ^ { N } } { \\sum _ { h ^ { \\prime } \\in \\mathcal { H } } p \\left( h ^ { \\prime } \\right) \\mathbb { I } \\left( \\mathcal { D } \\in h ^ { \\prime } \\right) / \\left| h ^ { \\prime } \\right| ^ { N } }$$ \n",
    "+ MAP (Maximum a posterior) estimate:\n",
    "$$\\hat { h } ^ { M A P } = \\operatorname { argmax } _ { h } p ( h | \\mathcal { D } ) = \\underset { h } { \\operatorname { argmax } } p ( \\mathcal { D } | h ) p ( h ) = \\underset { h } { \\operatorname { argmax } } [ \\log p ( \\mathcal { D } | h ) + \\log p ( h ) ]$$\n",
    "+ MLE (Maximum likelihood estimate):\n",
    "$$\\hat { h } ^ { m l e } \\triangleq \\underset { h } { \\operatorname { argmax } } p ( \\mathcal { D } | h ) = \\underset { h } { \\operatorname { argmax } } \\log p ( \\mathcal { D } | h )$$\n",
    "If we have enough data, the data overwhelms the prior => MAP convert to MLE\n",
    "  - If the true hypothesis is in the hypothesis space, the MAP/ML estimate will converge upon this hypothesis => Bayesian inference and ML estimation are consistent estimators => Hypothesis space is identifiable in the limit.\n",
    "\n",
    "  - If our hypothesis class is not rich enough to represent the truth, we will converge on the hypothesis that is as close as possible to the truth\n",
    "\n",
    "+ Posterior predictive distribution: justify the posterior (belief)\n",
    "$$p ( \\tilde { x } \\in C | \\mathcal { D } ) = \\sum _ { h } p ( y = 1 | \\tilde { x } , h ) p ( h | \\mathcal { D } )$$\n",
    "+ Bayes model averaging: weighted average of the predictions of each individual hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Beta-binomial model\n",
    "The probability that a coin shows up heads\n",
    "### Likelihood:\n",
    "$$X _ { i } \\sim \\operatorname { Ber } ( \\theta )$$\n",
    "$$p ( \\mathcal { D } | \\theta ) = \\theta ^ { N _ { 1 } } ( 1 - \\theta ) ^ { N _ { 0 } }$$\n",
    "\n",
    "sufficient statistics: $s(D)$ for data $D$ if $p ( \\boldsymbol { \\theta } | \\mathcal { D } ) = p ( \\boldsymbol { \\theta } | \\mathbf { s } ( \\text {data} ) )$\n",
    "$$N _ { 1 } = \\sum _ { i = 1 } ^ { N } \\mathbb { I } \\left( x _ { i } = 1 \\right)$$\n",
    "$$N _ { 0 } = \\sum _ { i = 1 } ^ { N } \\mathbb { I } \\left( x _ { i } = 0 \\right)$$\n",
    "if we have two datasets with the same sufficient statistics, we will infer the same value for $\\theta$\n",
    "\n",
    "### Prior\n",
    "choose conjugate prior (prior and the posterior have the same form):\n",
    "\n",
    "$$p ( \\theta ) \\propto \\theta ^ { \\gamma _ { 1 } } ( 1 - \\theta ) ^ { \\gamma _ { 2 } }$$\n",
    "$$p ( \\theta | \\mathcal { D }) \\propto p ( \\mathcal { D } | \\theta ) p ( \\theta ) = \\theta ^ { N _ { 1 } } ( 1 - \\theta ) ^ { N _ { 0 } } \\theta ^ { \\gamma _ { 1 } } ( 1 - \\theta ) ^ { \\gamma _ { 2 } } = \\theta ^ { N _ { 1 } + \\gamma _ { 1 } } ( 1 - \\theta ) ^ { N _ { 0 } + \\gamma _ { 2 } }$$\n",
    " \n",
    "in Bernoulli, conjugate is beta distribution:\n",
    "$$\\operatorname { Beta } ( \\theta | a , b ) \\propto \\theta ^ { a - 1 } ( 1 - \\theta ) ^ { b - 1 }$$\n",
    "\n",
    "uninformative piror = uniform piror, a = b = 1\n",
    "\n",
    "### Posterior\n",
    "$$p ( \\theta | \\mathcal { D } ) \\propto \\operatorname { Bin } \\left( N _ { 1 } | \\theta , N _ { 0 } + N _ { 1 } \\right) \\operatorname { Beta } ( \\theta | a , b ) \\operatorname { Beta } ( \\theta | N _ { 1 } + a , N _ { 0 } + b )$$\n",
    "\n",
    "$$\\hat { \\theta } _ { M A P } = \\frac { a + N _ { 1 } - 1 } { a + b + N - 2 }$$\n",
    "$$\\hat { \\theta } _ { M L E } = \\frac { N _ { 1 } } { N }$$\n",
    "$$\\overline { \\theta } = \\frac { a + N _ { 1 } } { a + b + N }$$\n",
    "\n",
    "### Posterior predictive distribution\n",
    "$$\\begin{aligned} p ( \\tilde { x } = 1 | \\mathcal { D } ) & = \\int _ { 0 } ^ { 1 } p ( x = 1 | \\theta ) p ( \\theta | \\mathcal { D } ) d \\theta \\\\ & = \\int _ { 0 } ^ { 1 } \\theta \\operatorname { Beta } ( \\theta | a , b ) d \\theta = \\mathbb { E } [ \\theta | \\mathcal { D } ] = \\frac { a } { a + b } \\end{aligned}$$\n",
    "\n",
    "+ zero count problem or sparse data problem or black swan paradox (something that couldn't exist), add-one smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Dirichlet-multinomial model\n",
    "the probability that a dice with $K$ sides comes up as face $k$\n",
    "\n",
    "### Likelihood\n",
    "$$p ( \\mathcal { D } | \\boldsymbol { \\theta } ) = \\prod _ { k = 1 } ^ { K } \\theta _ { k } ^ { N _ { k } }$$\n",
    "\n",
    "$$N _ { k } = \\sum _ { i = 1 } ^ { N } \\mathbb { I } \\left( y _ { i } = k \\right) \\text { is the number of times event } k \\text { occured }$$\n",
    "\n",
    "### Prior\n",
    "conjugate prior is Dirichlet distribution\n",
    "$$\\operatorname { Dir } ( \\boldsymbol { \\theta } | \\boldsymbol { \\alpha } ) = \\frac { 1 } { B ( \\boldsymbol { \\alpha } ) } \\prod _ { k = 1 } ^ { K } \\theta _ { k } ^ { \\alpha _ { k } - 1 } \\mathbb { I } \\left( \\mathbf { x } \\in S _ { K } \\right)$$\n",
    "\n",
    "### Posterior\n",
    "$$\\begin{aligned} p ( \\boldsymbol { \\theta } | \\mathcal { D } ) & \\propto p ( \\mathcal { D } | \\boldsymbol { \\theta } ) p ( \\boldsymbol { \\theta } ) \\\\ & \\propto \\prod _ { k = 1 } ^ { K } \\theta _ { k } ^ { N _ { k } } \\theta _ { k } ^ { \\alpha _ { k } - 1 } = \\prod _ { k = 1 } ^ { K } \\theta _ { k } ^ { \\alpha _ { k } + N _ { k } - 1 } \\\\ & = \\operatorname { Dir } ( \\boldsymbol { \\theta } | \\alpha _ { 1 } + N _ { 1 } , \\ldots , \\alpha _ { K } + N _ { K } ) \\end{aligned}$$\n",
    "\n",
    "MAP estimate will find the solution for $\\theta$:\n",
    "$$\\hat { \\theta } _ { k } = \\frac { N _ { k } + \\alpha _ { k } - 1 } { N + \\alpha _ { 0 } - K }$$\n",
    "\n",
    "### Posterior Predictive\n",
    "$$\\begin{aligned} p ( X = j | \\mathcal { D } ) & = \\int p ( X = j | \\theta ) p ( \\boldsymbol { \\theta } | \\mathcal { D } ) d \\boldsymbol { \\theta } \\\\ & = \\int p ( X = j | \\theta _ { j } ) \\left[ \\int p \\left( \\boldsymbol { \\theta } _ { - j } , \\theta _ { j } | \\mathcal { D } \\right) d \\boldsymbol { \\theta } _ { - j } \\right] d \\theta _ { j } \\\\ & = \\int \\theta _ { j } p \\left( \\theta _ { j } | \\mathcal { D } \\right) d \\theta _ { j } = \\mathbb { E } \\left[ \\theta _ { j } | \\mathcal { D } \\right] = \\frac { \\alpha _ { j } + N _ { j } } { \\sum _ { k } \\left( \\alpha _ { k } + N _ { k } \\right) } = \\frac { \\alpha _ { j } + N _ { j } } { \\alpha _ { 0 } + N } \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-sum-exp trick\n",
    "$$\\log \\sum _ { c } e ^ { b _ { c } } = \\log \\left[ \\left( \\sum _ { c } e ^ { b _ { c } - B } \\right) e ^ { B } \\right] = \\left[ \\log \\left( \\sum _ { c } e ^ { b _ { c } - B } \\right) \\right] + B$$\n",
    "where $B = \\max_c b_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes Classifiers (NBC)\n",
    "\n",
    "### Model Specification\n",
    "The features are conditionaly independent given the class label\n",
    "$$p ( \\mathbf { x } | y = c , \\boldsymbol { \\theta } ) = \\prod _ { j = 1 } ^ { D } p \\left( x _ { j } | y = c , \\boldsymbol { \\theta } _ { j c } \\right)$$\n",
    "\n",
    "Some likelihood distributions:\n",
    "+ Gaussian: $$p ( \\mathbf { x } | y = c , \\boldsymbol { \\theta } ) = \\prod _ { j = 1 } ^ { D } \\mathcal { N } \\left( x _ { j } | \\mu _ { j c } , \\sigma _ { j c } ^ { 2 } \\right)$$\n",
    "+ Bernoulli: $$p ( \\mathbf { x } | y = c , \\boldsymbol { \\theta } ) = \\prod _ { j = 1 } ^ { D } \\operatorname { Ber } \\left( x _ { j } | \\mu _ { j c } \\right)$$\n",
    "+ Categorical: $$p ( \\mathbf { x } | y = c , \\boldsymbol { \\theta } ) = \\prod _ { j = 1 } ^ { D } \\operatorname { Cat } \\left( x _ { j } | \\boldsymbol { \\mu } _ { j c } \\right)$$\n",
    "\n",
    "### Likelihood: \n",
    "$$p \\left( \\mathbf { x } _ { i } , y _ { i } | \\boldsymbol { \\theta } \\right) = p \\left( y _ { i } | \\boldsymbol { \\pi } \\right) \\prod _ { j } p \\left( x _ { i j } | \\boldsymbol { \\theta } _ { j } \\right) = \\prod _ { c } \\pi _ { c } ^ { \\mathbb { I } \\left( y _ { i } = c \\right) } \\prod _ { j } \\prod _ { c } p \\left( x _ { i j } | \\boldsymbol { \\theta } _ { j c } \\right) ^ { \\mathbb { I } \\left( y _ { i } = c \\right) }$$\n",
    "\n",
    "Log-likehood: \n",
    "$$\\log p ( \\mathcal { D } | \\boldsymbol { \\theta } ) = \\sum _ { c = 1 } ^ { C } N _ { c } \\log \\pi _ { c } + \\sum _ { j = 1 } ^ { D } \\sum _ { c = 1 } ^ { C } \\sum _ { i : y _ { i } = c } \\log p \\left( x _ { i j } | \\boldsymbol { \\theta } _ { j c } \\right)$$\n",
    "\n",
    "==> MLE:\n",
    "$$\\hat { \\pi } _ { c } = \\frac { N _ { c } } { N }$$\n",
    "\n",
    "$$\\hat { \\theta } _ { j c } = \\frac { N _ { j c } } { N _ { c } }$$\n",
    "\n",
    "### Prior:\n",
    "$$p ( \\boldsymbol { \\theta } ) = p ( \\boldsymbol { \\pi } ) \\prod _ { j = 1 } ^ { D } \\prod _ { c = 1 } ^ { C } p \\left( \\theta _ { j c } \\right)$$\n",
    "\n",
    "We use $Dir(\\alpha)$ prior for $\\pi$ and $Beta(\\beta_0, \\beta_1)$ for each $\\theta_{jc}$ (normally $\\alpha=1, \\beta=1$)\n",
    "\n",
    "### Posterior:\n",
    "$$\\begin{aligned} p ( \\boldsymbol { \\theta } | \\mathcal { D } ) & = p ( \\boldsymbol { \\pi } | \\mathcal { D } ) \\prod _ { j = 1 } ^ { D } \\prod _ { c = } ^ { C } p \\left( \\theta _ { j c } | \\mathcal { D } \\right) \\\\ p ( \\boldsymbol { \\pi } | \\mathcal { D } ) & = \\operatorname { Dir } \\left( N _ { 1 } + \\alpha _ { 1 } \\ldots , N _ { C } + \\alpha _ { C } \\right) \\\\ p \\left( \\theta _ { j c } | \\mathcal { D } \\right) & = \\operatorname { Beta } \\left( \\left( N _ { c } - N _ { j c } \\right) + \\beta _ { 0 } , N _ { j c } + \\beta _ { 1 } \\right) \\end{aligned}$$\n",
    "\n",
    "### Posterior Predictive\n",
    "$$p ( y = c | \\mathbf { x } , \\mathcal { D } ) \\quad \\propto \\quad p ( y = c | \\mathcal { D } ) \\prod _ { j = 1 } ^ { D } p \\left( x _ { j } | y = c , \\mathcal { D } \\right)$$\n",
    "\n",
    "$$\\begin{aligned} p ( y = c | \\mathbf { x } , \\mathcal { D } ) & \\propto \\overline { \\pi } _ { c } \\prod _ { j = 1 } ^ { D } \\left( \\overline { \\theta } _ { j c } \\right) ^ { \\mathbb { I } \\left( x _ { j } = 1 \\right) } \\left( 1 - \\overline { \\theta } _ { j c } \\right) ^ { \\mathbb { I } \\left( x _ { j } = 0 \\right) } \\\\ \\overline { \\theta } _ { j k } & = \\frac { N _ { j c } + \\beta _ { 1 } } { N _ { c } + \\beta _ { 0 } + \\beta _ { 1 } } \\\\ \\overline { \\pi } _ { c } & = \\frac { N _ { c } + \\alpha _ { c } } { N + \\alpha _ { 0 } } \\\\\n",
    "\\alpha _ { 0 } & = \\sum _ { c } \\alpha _ { c }\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
