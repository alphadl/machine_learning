{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "## Fundamental rules:\n",
    "+ Probability of a union of two events:\n",
    "$$\\begin{aligned} p ( A \\vee B ) & = p ( A ) + p ( B ) - p ( A \\wedge B ) \\\\ & = p ( A ) + p ( B ) \\text { if } A \\text { and } B \\text { are mutually exclusive } \\end{aligned}$$\n",
    "\n",
    "+ Joint probability (product rule):\n",
    "$$p ( A , B ) = p ( A \\wedge B ) = p ( A | B ) p ( B )$$\n",
    "\n",
    "+ Marginal distribution (sum rule, rule of total probability):\n",
    "$$p ( A ) = \\sum _ { b } p ( A , B ) = \\sum _ { b } p ( A | B = b ) p ( B = b )$$\n",
    "\n",
    "+ Chain rule:\n",
    "$$p \\left( X _ { 1 : D } \\right) = p \\left( X _ { 1 } \\right) p \\left( X _ { 2 } | X _ { 1 } \\right) p \\left( X _ { 3 } | X _ { 2 } , X _ { 1 } \\right) p \\left( X _ { 4 } | X _ { 1 } , X _ { 2 } , X _ { 3 } \\right) \\ldots p \\left( X _ { D } | X _ { 1 : D - 1 } \\right)$$\n",
    "\n",
    "+ Conditional Probability:\n",
    "$$p ( A | B ) = \\frac { p ( A , B ) } { p ( B ) } \\text { if } p ( B ) > 0$$\n",
    "\n",
    "+ Bayes rule: \n",
    "$$p ( X = x | Y = y ) = \\frac { p ( X = x , Y = y ) } { p ( Y = y ) } = \\frac { p ( X = x ) p ( Y = y | X = x ) } { \\sum _ { x ^ { \\prime } } p \\left( X = x ^ { \\prime } \\right) p ( Y = y | X = x ^ { \\prime } ) }$$\n",
    "\n",
    "+ Independence: \n",
    "$$X \\perp Y \\Longleftrightarrow p ( X , Y ) = p ( X ) p ( Y )$$\n",
    "\n",
    "+ Conditional Independence:\n",
    "$$X \\perp Y | Z \\Longleftrightarrow p ( X , Y | Z ) = p ( X | Z ) p ( Y | Z )$$\n",
    "\n",
    "+ Expectation:\n",
    "$$E_{p(x)}[f(x)] = \\sum_x f(x)P(x)$$\n",
    "$$E_{p(x)}[f(x)] = \\int f(x)P(x)dx$$\n",
    "\n",
    "+ Covariance:\n",
    "$$\\operatorname { cov } [ X , Y ] \\triangleq \\mathbb { E } [ ( X - \\mathbb { E } [ X ] ) ( Y - \\mathbb { E } [ Y ] ) ] = \\mathbb { E } [ X Y ] - \\mathbb { E } [ X ] \\mathbb { E } [ Y ]$$\n",
    "\n",
    "+ Pearson correlation coefficient (normalized Covariance)\n",
    "$$\\operatorname { corr } [ X , Y ] \\triangleq \\frac { \\operatorname { cov } [ X , Y ] } { \\sqrt { \\operatorname { var } [ X ] \\operatorname { var } [ Y ] } }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some common discrete distributions\n",
    "\n",
    "+ Binomial distribution: outcome of tossing a coin (2 sides) n times\n",
    "$$\\operatorname { Bin } ( k | n , \\theta ) \\triangleq \\left( \\begin{array} { l } { n } \\\\ { k } \\end{array} \\right) \\theta ^ { k } ( 1 - \\theta ) ^ { n - k }$$\n",
    "\n",
    "where: n choose k $\\left( \\begin{array} { l } { n } \\\\ { k } \\end{array} \\right) \\triangleq \\frac { n ! } { ( n - k ) ! k ! }$\n",
    "\n",
    "$$\\text { mean } = \\theta , \\quad \\text { var } = n \\theta ( 1 - \\theta )$$\n",
    "\n",
    "+ Bernoulli distribution: utcome of tossing a coin 1 time\n",
    "$$\\operatorname { Ber } ( x | \\theta ) = \\theta ^ { \\mathbb { I } ( x = 1 ) } ( 1 - \\theta ) ^ { \\mathbb { I } ( x = 0 ) }$$ \n",
    "In other words,\n",
    "$$\\operatorname { Ber } ( x | \\theta ) = \\left\\{ \\begin{array} { l l } { \\theta } & { \\text { if } x = 1 } \\\\ { 1 - \\theta } & { \\text { if } x = 0 } \\end{array} \\right.$$\n",
    "\n",
    "+ Multinomial distribution: outcome of tossing a K-sided die n times \n",
    "$$\\operatorname { Mu } ( \\mathbf { x } | n , \\boldsymbol { \\theta } ) \\triangleq \\left( \\begin{array} { c } { n } \\\\ { x _ { 1 } \\ldots x _ { K } } \\end{array} \\right) \\prod _ { j = 1 } ^ { K } \\theta _ { j } ^ { x _ { j } }$$\n",
    "\n",
    "where $\\theta_j$ is the probability taht side $j$ shows up and:\n",
    "$\\left( \\begin{array} { c } { n } \\\\ { x _ { 1 } \\ldots x _ { K } } \\end{array} \\right) \\triangleq \\frac { n ! } { x _ { 1 } ! x _ { 2 } ! \\cdots x _ { K } ! }$\n",
    "\n",
    "+ Multinoulli (Categorical) distribution: outcome of tossing a K-sided die 1 time\n",
    "$$\\operatorname { Cat } ( x | \\boldsymbol { \\theta } ) \\triangleq \\operatorname { Mu } ( \\mathbf { x } | 1 , \\boldsymbol { \\theta } ) = \\prod _ { j = 1 } ^ { K } \\theta _ { j } ^ { \\mathbb { I } \\left( x _ { j } = 1 \\right) }$$\n",
    "\n",
    "+ Poisson distribution: used for counting rare events like radioactive decay, traffic accidents $X \\in \\{0, 1, 2, \\ldots\\}$\n",
    "$$\\operatorname { Poi } ( x | \\lambda ) = e ^ { - \\lambda } \\frac { \\lambda ^ { x } } { x ! }$$\n",
    "\n",
    "+ Emperical distribution:\n",
    "Given dataset $D = \\{x_1, \\ldots, x_N\\}$:\n",
    "$$p _ { \\mathrm { emp } } ( A ) \\triangleq \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\delta _ { x _ { i } } ( A )$$\n",
    "\n",
    "where $\\delta_x(A)$ is Dirac measure: $\\delta _ { x } ( A ) = \\left\\{ \\begin{array} { l l } { 0 } & { \\text { if } x \\notin A } \\\\ { 1 } & { \\text { if } x \\in A } \\end{array} \\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some common continous distributions\n",
    "+ Uniform distribution:\n",
    "$$\\operatorname { Unif } ( x | a , b ) = \\frac { 1 } { b - a } \\mathbb { I } ( a \\leq x \\leq b )$$\n",
    "\n",
    "+ Gaussian (normal) distribution: most widely used distribution in statistics\n",
    "$$\\mathcal { N } ( x | \\mu , \\sigma ^ { 2 } ) \\triangleq \\frac { 1 } { \\sqrt { 2 \\pi \\sigma ^ { 2 } } } e ^ { - \\frac { 1 } { 2 \\sigma ^ { 2 } } ( x - \\mu ) ^ { 2 } }$$\n",
    "\n",
    "where $\\mu = E[X]$ is the mean, $\\sigma^2 = Var[X]$ is the variance and $\\lambda = 1/\\sigma^2$ is the precision. A high precision means a narrow distribution (low variance) centered on $\\mu$\n",
    "\n",
    "  **cdf**: $$\\Phi \\left( x ; \\mu , \\sigma ^ { 2 } \\right) \\triangleq \\int _ { - \\infty } ^ { x } \\mathcal { N } ( z | \\mu , \\sigma ^ { 2 } ) d z$$\n",
    "\n",
    "  **error function (erf)**: $$\\Phi ( x ; \\mu , \\sigma ) = \\frac { 1 } { 2 } [ 1 + \\operatorname { erf } ( z / \\sqrt { 2 } ) ]$$\n",
    "\n",
    "where $z = (x - \\mu)/\\sigma$ and $\\operatorname { erf } ( x ) \\triangleq \\frac { 2 } { \\sqrt { \\pi } } \\int _ { 0 } ^ { x } e ^ { - t ^ { 2 } } d t$\n",
    " \n",
    "  **central limit theorem**: sums of independent random variables have an approximately Gaussian distribution\n",
    "\n",
    "+ Student t distribution: (more robust than normal distribution with outlier)\n",
    "$$\\mathcal { T } ( x | \\mu , \\sigma ^ { 2 } , \\nu ) \\propto \\left[ 1 + \\frac { 1 } { \\nu } \\left( \\frac { x - \\mu } { \\sigma } \\right) ^ { 2 } \\right] ^ { - \\left( \\frac { \\nu + 1 } { 2 } \\right) }$$\n",
    "\n",
    "where $\\mu$ is the location, $\\sigma > 0$ is the scale parameter, and $\\nu > 0$ is the degrees of freedom: \n",
    "$$\\operatorname { mean } = \\mu , \\operatorname { mode } = \\mu , \\operatorname { var } = \\frac { \\nu \\sigma ^ { 2 } } { ( \\nu - 2 ) }$$\n",
    "\n",
    "+ The laplace distribution (with heavy tails, or double sided exponential distribution)\n",
    "$$\\operatorname { Lap } ( x | \\mu , b ) \\triangleq \\frac { 1 } { 2 b } \\exp \\left( - \\frac { | x - \\mu | } { b } \\right)$$\n",
    "\n",
    "where $\\mu$ is the location, $b > 0$ is the scale parameter: $$\\text { mean } = \\mu , \\text { mode } = \\mu , \\text { var } = 2 b ^ { 2 }$$\n",
    "\n",
    "![Gaussian, Student-t and laplace, Gamma](../images/2.gaussian.png)\n",
    "\n",
    "+ The gamma distribution: for $x > 0$:\n",
    "$$\\mathrm { Ga } ( T | \\text { shape } = a , \\text { rate } = b ) \\triangleq \\frac { b ^ { a } } { \\Gamma ( a ) } T ^ { a - 1 } e ^ { - T b }$$\n",
    "\n",
    "where $\\Gamma(a)$ is the gamma function: $\\Gamma ( x ) \\triangleq \\int _ { 0 } ^ { \\infty } u ^ { x - 1 } e ^ { - u } d u$\n",
    "\n",
    "$$\\text { mean } = \\frac { a } { b } , \\text { mode } = \\frac { a - 1 } { b } , \\text { var } = \\frac { a } { b ^ { 2 } }$$\n",
    "\n",
    "+ The beta distribution: for $x \\in [0, 1]$\n",
    "$$\\operatorname { Beta } ( x | a , b ) = \\frac { 1 } { B ( a , b ) } x ^ { a - 1 } ( 1 - x ) ^ { b - 1 }$$\n",
    "\n",
    "where $Beta(p,q)$ is beta function: $B ( a , b ) \\triangleq \\frac { \\Gamma ( a ) \\Gamma ( b ) } { \\Gamma ( a + b ) }$\n",
    "$$\\text { mean } = \\frac { a } { a + b } , \\text { mode } = \\frac { a - 1 } { a + b - 2 } , \\text { var } = \\frac { a b } { ( a + b ) ^ { 2 } ( a + b + 1 ) }$$\n",
    "\n",
    "![Beta](../images/2.gamma_beta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Probability Distributions\n",
    "\n",
    "+ Multivariate Gaussian distribution (MVN):\n",
    "The pdf:\n",
    "$$\\mathcal { N } ( \\mathbf { x } | \\boldsymbol { \\mu } , \\mathbf { \\Sigma } ) \\triangleq \\frac { 1 } { ( 2 \\pi ) ^ { D / 2 } | \\mathbf { \\Sigma } | ^ { 1 / 2 } } \\exp \\left[ - \\frac { 1 } { 2 } ( \\mathbf { x } - \\boldsymbol { \\mu } ) ^ { T } \\mathbf { \\Sigma } ^ { - 1 } ( \\mathbf { x } - \\boldsymbol { \\mu } ) \\right]$$\n",
    "\n",
    "where $ { \\mu } = \\mathbb { E } [ \\mathbf { x } ] \\in \\mathbb { R } ^ { D } $ is the mean vector, and $ { \\Sigma } = \\operatorname { cov } [ \\mathbf { x } ] $ is the $D \\times D $ covariance matrix, precision matrix: $\\mathbf { \\Lambda } = \\mathbf { \\Sigma } ^ { - 1 }$\n",
    "\n",
    "+ Multivariate Student-t distribution (robust version of MVN):\n",
    "$$\\begin{aligned} \\mathcal { T } ( \\mathbf { x } | \\boldsymbol { \\mu } , \\mathbf { \\Sigma } , \\nu ) & = \\frac { \\Gamma ( \\nu / 2 + D / 2 ) } { \\Gamma ( \\nu / 2 ) } \\frac { | \\mathbf { \\Sigma } | ^ { - 1 / 2 } } { \\nu ^ { D / 2 } \\pi ^ { D / 2 } } \\times \\left[ 1 + \\frac { 1 } { \\nu } ( \\mathbf { x } - \\boldsymbol { \\mu } ) ^ { T } \\mathbf { \\Sigma } ^ { - 1 } ( \\mathbf { x } - \\boldsymbol { \\mu } ) \\right] ^ { - \\left( \\frac { \\nu + D } { 2 } \\right) } \\\\ & = \\frac { \\Gamma ( \\nu / 2 + D / 2 ) } { \\Gamma ( \\nu / 2 ) } | \\pi \\mathbf { V } | ^ { - 1 / 2 } \\times \\left[ 1 + ( \\mathbf { x } - \\boldsymbol { \\mu } ) ^ { T } \\mathbf { V } ^ { - 1 } ( \\mathbf { x } - \\boldsymbol { \\mu } ) \\right] ^ { - \\left( \\frac { \\nu + D } { 2 } \\right) } \\end{aligned}$$\n",
    "\n",
    "$$\\text { mean } = \\mu , \\text { mode } = \\mu , \\quad \\text { Cov } = \\frac { \\nu } { \\nu - 2 } \\Sigma$$\n",
    "\n",
    "+ Dirichlet distribution (Multivariate Beta distribution): support over probability simplex: $S _ { K } = \\left\\{ \\mathbf { x } : 0 \\leq x _ { k } \\leq 1 , \\sum _ { k = 1 } ^ { K } x _ { k } = 1 \\right\\}$\n",
    "\n",
    "$$\\operatorname { Dir } ( \\mathbf { x } | \\boldsymbol { \\alpha } ) \\triangleq \\frac { 1 } { B ( \\boldsymbol { \\alpha } ) } \\prod _ { k = 1 } ^ { K } x _ { k } ^ { \\alpha _ { k } - 1 } \\mathbb { I } \\left( \\mathbf { x } \\in S _ { K } \\right)$$\n",
    "\n",
    "Properties: $$\\mathbb { E } \\left[ x _ { k } \\right] = \\frac { \\alpha _ { k } } { \\alpha _ { 0 } } , \\operatorname { mode } \\left[ x _ { k } \\right] = \\frac { \\alpha _ { k } - 1 } { \\alpha _ { 0 } - K } , \\operatorname { var } \\left[ x _ { k } \\right] = \\frac { \\alpha _ { k } \\left( \\alpha _ { 0 } - \\alpha _ { k } \\right) } { \\alpha _ { 0 } ^ { 2 } \\left( \\alpha _ { 0 } + 1 \\right) }$$\n",
    "\n",
    "![MVN](../images/2.MVN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of Random Variables\n",
    "\n",
    "if $\\mathbf { x } \\sim p ( )$ is some random variable, and $y = f(x)$, we find distribution of $y$\n",
    "\n",
    "+ Linear transformation:\n",
    "$$\\mathbf { y } = f ( \\mathbf { x } ) = \\mathbf { A x } + \\mathbf { b }$$\n",
    "\n",
    "mean: $$\\mathbb { E } [ \\mathbf { y } ] = \\mathbb { E } [ \\mathbf { A } \\mathbf { x } + \\mathbf { b } ] = \\mathbf { A } \\boldsymbol { \\mu } + \\mathbf { b }$$ where $\\mu = \\mathbb {E}[x]$: linear expectation\n",
    "\n",
    "covariance: $$\\operatorname { cov } [ \\mathbf { y } ] = \\operatorname { cov } [ \\mathbf { A } \\mathbf { x } + \\mathbf { b } ] = \\mathbf { A } \\boldsymbol { \\Sigma } \\mathbf { A } ^ { T }$$\n",
    "\n",
    "+ General transformation:\n",
    "change of variables formula:\n",
    "$$p _ { y } ( y ) = p _ { x } ( x ) \\left| \\frac { d x } { d y } \\right|$$\n",
    "\n",
    "log form:\n",
    "$$\\log p _ { y } ( y ) = \\log p _ { x } ( x )  + \\log \\left| \\frac { d x } { d y } \\right|$$ \n",
    "\n",
    "+ Monte Carlo approximation (instead of change of variables): \n",
    "we generate $S$ samples from the distribution, $x_1, \\ldots, x_S$, we can approximate the distribution of $f(X)$ by using the emperical distribution of $\\left\\{ f \\left( x _ { s } \\right) \\right\\} _ { s = 1 } ^ { S }$\n",
    "\n",
    "We can use Monte Carlo to approximate the expected value of any function of a random variable. We simply draw samples, and then compute the arithmetic mean of the function applied to the samples.\n",
    "\n",
    "$$\\mathbb { E } [ f ( X ) ] = \\int f ( x ) p ( x ) d x \\approx \\frac { 1 } { S } \\sum _ { s = 1 } ^ { S } f \\left( x _ { s } \\right)$$\n",
    "\n",
    "where $x_s \\sim p(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information theory\n",
    "\n",
    "+ Entropy: of variable $X$ measures its uncertainty.\n",
    "$$\\mathbb { H } ( X ) \\triangleq - \\sum _ { k = 1 } ^ { K } p ( X = k ) \\log _ { 2 } p ( X = k )$$\n",
    "\n",
    "Binary entropy function: \n",
    "$$\\begin{aligned} \\mathbb { H } ( X ) & = - \\left[ p ( X = 1 ) \\log _ { 2 } p ( X = 1 ) + p ( X = 0 ) \\log _ { 2 } p ( X = 0 ) \\right] \\\\ & = - \\left[ \\theta \\log _ { 2 } \\theta + ( 1 - \\theta ) \\log _ { 2 } ( 1 - \\theta ) \\right] \\end{aligned}$$\n",
    "\n",
    "+ KL divergence (relative entropy): measures the dissimilarity of two probability distribution $p$ and $q$ \n",
    "$$\\mathbb { K } \\mathbb { L } ( p \\| q ) \\triangleq \\sum _ { k = 1 } ^ { K } p _ { k } \\log \\frac { p _ { k } } { q _ { k } }$$ \n",
    "$$\\mathbb { K } \\mathbb { L } ( p \\| q ) = \\sum _ { k } p _ { k } \\log p _ { k } - \\sum _ { k } p _ { k } \\log q _ { k } = - \\mathbb { H } ( p ) + \\mathbb { H } ( p , q )$$\n",
    "\n",
    "where $H(p,q)$ is cross entropy: $\\mathbb { H } ( p , q ) \\triangleq - \\sum _ { k } p _ { k } \\log q _ { k }$: averge number of bits needed to encode data coming from a source with distribution $p$ when we use model $q$ to define our codebook.\n",
    "\n",
    "==> KL diverergence = the average number of extra bits needed to encode the data, due to the fact that we used distribution $q$ to encode the data instead of the true distribution $p$\n",
    "\n",
    "+ Mutual information (MI): How much knowing one variable tells us about the other:\n",
    "$$\\mathbb { I } ( X ; Y ) \\triangleq \\mathbb { K } \\mathbb { L } ( p ( X , Y ) \\| p ( X ) p ( Y ) ) = \\sum _ { x } \\sum _ { y } p ( x , y ) \\log \\frac { p ( x , y ) } { p ( x ) p ( y ) }$$\n",
    "\n",
    "MI = 0 iff the variable are independent\n",
    "$$\\mathbb { I } ( X ; Y ) = \\mathbb { H } ( X ) - \\mathbb { H } ( X | Y ) = \\mathbb { H } ( Y ) - \\mathbb { H } ( Y | X )$$\n",
    "\n",
    "where $ \\mathbb { H } ( Y | X )$ is conditional entropy: $\\mathbb { H } ( Y | X ) = \\sum _ { x } p ( x ) \\mathbb { H } ( Y | X = x )$\n",
    "\n",
    "+ Pointwise mutual information (PMI): of two events (not random variables)\n",
    "$$\\operatorname { PMI } ( x , y ) \\triangleq \\log \\frac { p ( x , y ) } { p ( x ) p ( y ) } = \\log \\frac { p ( x | y ) } { p ( x ) } = \\log \\frac { p ( y | x ) } { p ( y ) }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
