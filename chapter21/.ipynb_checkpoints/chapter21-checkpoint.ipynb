{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "The basic idea is to pick an approximation $q(x)$ to the distribution from some tractable family, and then to try to make this approximation as close as possible to the true posterior $p ^ { * } ( \\mathbf { x } ) \\triangleq p ( \\mathbf { x } | \\mathcal { D } )$. This reduces inference to an optimization problem. By relaxing the constraints and/or approximating the objective, we can trade accuracy for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference\n",
    "\n",
    "Suppose $p^∗(x)$ is our true but intractable distribution and $q(x)$ is some approximation, chosen from some tractable family, such as a multivariate Gaussian or a factored distribution. We assume $q$ has some free parameters which we want to optimize so as to make $q$ “similar to” $p^∗$.\n",
    "\n",
    "KL divergence:\n",
    "$$\\mathbb { K } \\mathbb { L } ( q \\| p ^ { * } ) = \\sum _ { \\mathbf { x } } q ( \\mathbf { x } ) \\log \\frac { q ( \\mathbf { x } ) } { p ^ { * } ( \\mathbf { x } ) }$$\n",
    "\n",
    "Evaluating $p ^ { * } ( \\mathbf { x } ) \\triangleq p ( \\mathbf { x } | \\mathcal { D } )$ is intractable since it requires evaluating the intractable normalization constant $Z = p(\\mathcal{D})$. However, usually the unormalized distribution $\\tilde { p } ( \\mathbf { x } ) \\triangleq p ( \\mathbf { x } , \\mathcal { D } ) = p ^ { * } ( \\mathbf { x } ) Z$ is tractable to compute. \n",
    "\n",
    "Objective function:\n",
    "$$J ( q ) \\triangleq \\mathbb { K } \\mathbb { L } ( q \\| \\tilde { p } ) = \\mathbb { K } \\mathbb { L } ( q \\| p ^ { * } ) - \\log Z$$\n",
    "\n",
    "Since $Z$ is a constant, by minimizing $J(q)$, we will force $q$ to become close to $p^∗$. Since KL divergence is always non-negative, we see that $J(q)$ is an upper bound on the NLL:\n",
    "$$J ( q ) = \\mathbb { K } \\mathbb { L } ( q \\| p ^ { * } ) - \\log Z \\geq - \\log Z = - \\log p ( \\mathcal { D } )$$\n",
    "\n",
    "Equivalently, we have the following objective:\n",
    "\n",
    "$$\\begin{aligned} J ( q ) & = \\mathbb { E } _ { q } [ \\log q ( \\mathbf { x } ) - \\log p ( \\mathbf { x } ) p ( \\mathcal { D } | \\mathbf { x } ) ] \\\\ & = \\mathbb { E } _ { q } [ \\log q ( \\mathbf { x } ) - \\log p ( \\mathbf { x } ) - \\log p ( \\mathcal { D } | \\mathbf { x } ) ] \\\\ & = \\mathbb { E } _ { q } [ - \\log p ( \\mathcal { D } | \\mathbf { x } ) ] + \\mathbb { K } \\mathbb { L } ( q ( \\mathbf { x } ) | | p ( \\mathbf { x } ) ) \\end{aligned}$$\n",
    "\n",
    "This is the expected NLL, plus a penalty term that measures how far the approximate posterior is from the exact prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mean field method\n",
    "or the mean field approximation for inferring latent variable $z_i$. We assume the parameters $\\theta$ of the model are known.\n",
    "\n",
    "We assume the posterior is fully factorized approximation of the form:\n",
    "$$q ( \\mathbf { x } ) = \\prod_i q _ { i } \\left( \\mathbf { x } _ { i } \\right)$$\n",
    "\n",
    "Our goal is to solve this optimization problem:\n",
    "$$\\min _ { q _ { 1 } , \\ldots , q _ { D } } \\mathbb { K } \\mathbb { L } ( q \\| p )$$\n",
    "\n",
    "where we optimize over the parameters of each marginal distribution $q_i$.\n",
    "\n",
    "We derive the coordinate descent method, where at each step we make the following update:\n",
    "$$\\log q _ { j } \\left( \\mathbf { x } _ { j } \\right) = \\mathbb { E } _ { - q _ { j } } [ \\log \\tilde { p } ( \\mathbf { x } ) ] + \\text { const }$$\n",
    "\n",
    "where $\\tilde { p } ( \\mathbf { x } ) = p ( \\mathbf { x } , \\mathcal { D } )$ is the unnormalized posterior and $\\mathbb { E } _ { - q _ { j } } [ f ( \\mathbf { x } ) ]$ means to take the expectation over $f(x)$ w.r.t all variables except for $x_j$. For example,\n",
    "$$\\mathbb { E } _ { - q _ { 2 } } [ f ( \\mathbf { x } ) ] = \\sum _ { x _ { 1 } } \\sum _ { x _ { 3 } } q \\left( x _ { 1 } \\right) q _ { 3 } \\left( x _ { 3 } \\right) f \\left( x _ { 1 } , x _ { 2 } , x _ { 3 } \\right)$$\n",
    "\n",
    "When updating $q_j$ , we only need to reason about the variables which share a factor with $x_j$ , i.e., the terms in $j$’s Markov blanket; the other terms get absorbed into the constant term. Since we are replacing the neighboring values by their mean value, the method is known as mean ﬁeld.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayes and Variational Bayes EM\n",
    "We want to infer the parameters themselves. If we make a fully factorized approximation: $p ( \\boldsymbol { \\theta } | \\mathcal { D } ) \\approx \\prod _ { k } q \\left( \\boldsymbol { \\theta } _ { k } \\right)$, we have variational Bayes or VB\n",
    "\n",
    "If we want to infer both latent variables and parameters, and we make an approximation of the form $p \\left( \\boldsymbol { \\theta } , \\mathbf { z } _ { 1 : N } | \\mathcal { D } \\right) \\approx q ( \\boldsymbol { \\theta } ) \\prod _ { i } q _ { i } \\left( \\mathbf { z } _ { i } \\right)$, we get a method known as variational Bayes EM. $\\mathbf { z } _ { i } \\rightarrow \\mathbf { x } _ { i } \\leftarrow \\boldsymbol { \\theta }$\n",
    "\n",
    "+ In E step we infer the posterior over the latent variables, $p \\left( \\mathbf { z } _ { i } | \\mathbf { x } _ { i } , \\boldsymbol { \\theta } \\right)$ (mean field)\n",
    "+ In M step, we compute a point estimate of the parameters $\\theta$: $q ( \\boldsymbol { \\theta } | \\mathcal { D } )$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loopy belief propagation (LBP) for general graph\n",
    "$q$ is not required to be factorized\n",
    "\n",
    "Idea: we apply the belief propagation algorithm to the graph, even if it has loops. This method is simple and efficient, and often works well in practice, outperforming mean field. \n",
    "\n",
    "### LBP on pairwise models\n",
    "![](../images/22.LBP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
