{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification\n",
    "\n",
    "$$ p ( y | \\mathbf { x } , \\mathbf { w } ) = \\operatorname { Ber } ( y | \\operatorname { sigm } \\left( \\mathbf { w } ^ { T } \\mathbf { x } \\right) ) $$\n",
    "\n",
    "![Sigmoid function](../images/8.LR.png)\n",
    "\n",
    "## Model fitting\n",
    "### MLE\n",
    "Negative log-likelihood: (Binary Cross Entropy loss)\n",
    "$$\\begin{aligned} \\mathrm { NLL } ( \\mathbf { w } ) & = - \\sum _ { i = 1 } ^ { N } \\log \\left[ \\mu _ { i } ^ { \\mathbb { I } \\left( y _ { i } = 1 \\right) } \\times \\left( 1 - \\mu _ { i } \\right) ^ { \\mathbb { I } \\left( y _ { i } = 0 \\right) } \\right] \\\\ & = - \\sum _ { i = 1 } ^ { N } \\left[ y _ { i } \\log \\mu _ { i } + \\left( 1 - y _ { i } \\right) \\log \\left( 1 - \\mu _ { i } \\right) \\right] \\\\\n",
    " & = \\sum _ { i = 1 } ^ { N } \\log \\left( 1 + \\exp \\left( - \\tilde { y } _ { i } \\mathbf { w } ^ { T } \\mathbf { x } _ { i } \\right) \\right)\\end{aligned}$$\n",
    "\n",
    "We don't have close-form solution, so we solve that optimization problem by gradient descent\n",
    "$$\\boldsymbol { \\theta } _ { k + 1 } = \\boldsymbol { \\theta } _ { k } - \\eta _ { k } \\mathbf { g } _ { k }$$\n",
    "\n",
    "where $\\eta$ is learning rate, we can also use momentum term where $\\mu_k$ controls the importance of momentum term:\n",
    "$$\\boldsymbol { \\theta } _ { k + 1 } = \\boldsymbol { \\theta } _ { k } - \\eta _ { k } \\mathbf { g } _ { k } + \\mu _ { k } \\left( \\boldsymbol { \\theta } _ { k } - \\boldsymbol { \\theta } _ { k - 1 } \\right)$$\n",
    "\n",
    "Newton's method:\n",
    "$$\\boldsymbol { \\theta } _ { k + 1 } = \\boldsymbol { \\theta } _ { k } - \\eta _ { k } \\mathbf { H } _ { k } ^ { - 1 } \\mathbf { g } _ { k }$$\n",
    "\n",
    "**MAP L2-regularizatrion**:\n",
    "$$\\begin{aligned} f ^ { \\prime } ( \\mathbf { w } ) & = \\mathrm { NLL } ( \\mathbf { w } ) + \\lambda \\mathbf { w } ^ { T } \\mathbf { w } \\\\ \\mathbf { g } ^ { \\prime } ( \\mathbf { w } ) & = \\mathbf { g } ( \\mathbf { w } ) + \\lambda \\mathbf { w } \\\\ \\mathbf { H } ^ { \\prime } ( \\mathbf { w } ) & = \\mathbf { H } ( \\mathbf { w } ) + \\lambda \\mathbf { I } \\end{aligned}$$\n",
    "\n",
    "**Multi-class logistic regression** (Cross Entropy loss)\n",
    "$$p ( y = c | \\mathbf { x } , \\mathbf { W } ) = \\frac { \\exp \\left( \\mathbf { w } _ { c } ^ { T } \\mathbf { x } \\right) } { \\sum _ { c ^ { \\prime } = 1 } ^ { C } \\exp \\left( \\mathbf { w } _ { c ^ { \\prime } } ^ { T } \\mathbf { x } \\right) }$$\n",
    "\n",
    "$$l(w) = - \\sum _ { c = 1 } ^ { M } y _ { o , c } \\log \\left( p _ { o , c } \\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
