{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undirected Graphical models (Markov random fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "name: undirected graphical model (UGM), Markov random field (MRF) or Markov Network\n",
    "\n",
    "![](../images/19.UGM.png)\n",
    "\n",
    "### Conditional independence of UGMs\n",
    "+ Global Markov property: $\\mathbf { x } _ { A } \\perp_G \\mathbf { X } _ { B } \\left| \\mathbf { x } _ { C }\\right.$, e.g.: $1 \\perp 7 | \\mathrm { rest }$\n",
    "+ Local Markov Property: $t \\perp \\mathcal { V } \\backslash \\operatorname { cl } ( t ) | \\mathrm { mb } ( t )$, where closure: $\\mathrm { cl } ( t ) \\triangleq \\mathrm { mb } ( t ) \\cup \\{ t \\}$, e.g.: $1 \\perp \\text { rest } | 2,3$\n",
    "+ Pairwise Markov Property: $s \\perp t \\left| \\mathcal { V } \\backslash \\{ s , t \\} \\Longleftrightarrow G _ { s t } = 0\\right.$, e.g.: $1,2 \\perp 6,7 | 3,4,5$\n",
    "\n",
    "### Comparing directed and undirected graphical models:\n",
    "![](../images/19.Compare.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterization of MRFs\n",
    "Representing joint distribution for a UGM:\n",
    "\n",
    "+ The Hammersley-Clifford theorem:\n",
    "\n",
    "    A positive distribution p(y) > 0 satisﬁes the CI properties of an undirected graph G iff p can be represented as a product of factors, one per maximal clique, i.e.,\n",
    "\n",
    "    $$p ( \\mathbf { y } | \\boldsymbol { \\theta } ) = \\frac { 1 } { Z ( \\boldsymbol { \\theta } ) } \\prod _ { c \\in \\mathcal { C } } \\psi _ { c } \\left( \\mathbf { y } _ { c } | \\boldsymbol { \\theta } _ { c } \\right)$$\n",
    "\n",
    "    where potential or factors for clique $c$ by: $\\psi _ { c } \\left( \\mathbf { y } _ { c } | \\boldsymbol { \\theta } _ { c } \\right) \\geq 0$, $C$ is the set of all the (maximal) cliques of $G$ and $Z(\\theta)$ is the partition function ensures that the overall distribution sums to 1:\n",
    "\n",
    "    $$Z ( \\boldsymbol { \\theta } ) \\triangleq \\sum _ { \\mathbf { x } } \\prod _ { c \\in \\mathcal { C } } \\psi _ { c } \\left( \\mathbf { y } _ { c } | \\boldsymbol { \\theta } _ { c } \\right)$$\n",
    "\n",
    "    Gibbs distribution:\n",
    "    $$p ( \\mathbf { y } | \\boldsymbol { \\theta } ) = \\frac { 1 } { Z ( \\boldsymbol { \\theta } ) } \\exp \\left( - \\sum _ { c } E \\left( \\mathbf { y } _ { c } | \\boldsymbol { \\theta } _ { c } \\right) \\right)$$\n",
    "\n",
    "    where $E(y_c) > 0$ is the energy associated with the variables in clique $c$, $\\psi _ { c } \\left( \\mathbf { y } _ { c } | \\boldsymbol { \\theta } _ { c } \\right) = \\exp \\left( - E \\left( \\mathbf { y } _ { c } | \\boldsymbol { \\theta } _ { c } \\right) \\right)$\n",
    "\n",
    "    High probability states correspond to low energy configurations ==> energy based models. \n",
    "\n",
    "    ![](../images/10.PGM.png)\n",
    "\n",
    "    $$p ( \\mathbf { y } | \\boldsymbol { \\theta } ) = \\frac { 1 } { Z ( \\boldsymbol { \\theta } ) } \\psi _ { 123 } \\left( y _ { 1 } , y _ { 2 } , y _ { 3 } \\right) \\psi _ { 234 } \\left( y _ { 2 } , y _ { 3 } , y _ { 4 } \\right) \\psi _ { 35 } \\left( y _ { 3 } , y _ { 5 } \\right)$$\n",
    "\n",
    "    where: $$Z = \\sum _ { \\mathbf { y } } \\psi _ { 123 } \\left( y _ { 1 } , y _ { 2 } , y _ { 3 } \\right) \\psi _ { 234 } \\left( y _ { 2 } , y _ { 3 } , y _ { 4 } \\right) \\psi _ { 35 } \\left( y _ { 3 } , y _ { 5 } \\right)$$\n",
    "\n",
    "    Pairwise MRF: restrict parameterization to the edges of the graph rather than the maximal cliques\n",
    "\n",
    "    $$\\begin{aligned} p ( \\mathbf { y } | \\boldsymbol { \\theta } ) & \\propto \\psi _ { 12 } \\left( y _ { 1 } , y _ { 2 } \\right) \\psi _ { 13 } \\left( y _ { 1 } , y _ { 3 } \\right) \\psi _ { 23 } \\left( y _ { 2 } , y _ { 3 } \\right) \\psi _ { 24 } \\left( y _ { 2 } , y _ { 4 } \\right) \\psi _ { 34 } \\left( y _ { 3 } , y _ { 4 } \\right) \\psi _ { 35 } \\left( y _ { 3 } , y _ { 5 } \\right) \\\\ & \\propto \\prod _ { s \\sim t } \\psi _ { s t } \\left( y _ { s } , y _ { t } \\right) \\end{aligned}$$\n",
    "\n",
    "+ Representing potential functions:\n",
    "    log potentials: $$\\log \\psi _ { c } \\left( \\mathbf { y } _ { c } \\right) \\triangleq \\boldsymbol { \\phi } _ { c } \\left( \\mathbf { y } _ { c } \\right) ^ { T } \\boldsymbol { \\theta } _ { c }$$\n",
    "\n",
    "    where $\\boldsymbol { \\phi } _ { c } \\left( \\mathbf { y } _ { c } \\right)$ is a feature vector derived from the values of the variables $y_c$\n",
    "\n",
    "    ==> log probability (**Maximum entropy, log-linear model**): $$\\log p ( \\mathbf { y } | \\boldsymbol { \\theta } ) = \\sum _ { c } \\boldsymbol { \\phi } _ { c } \\left( \\mathbf { y } _ { c } \\right) ^ { T } \\boldsymbol { \\theta } _ { c } - Z ( \\boldsymbol { \\theta } )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of MRFs:\n",
    "\n",
    "### Ising model:\n",
    "+ Pairwise clique potential: \n",
    "$$\\psi _ { s t } \\left( y _ { s } , y _ { t } \\right) = \\left( \\begin{array} { c c } { e ^ { w _ { s t } } } & { e ^ { - w _ { s t } } } \\\\ { e ^ { - w _ { s t } } } & { e ^ { w _ { s t } } } \\end{array} \\right)$$\n",
    "\n",
    "$w_{st}$ is the coupling strength between nodes $s$ and $t$. We assume all edges have the same strength: $w_{st} = J$. \n",
    "\n",
    "+ Assume $y_t \\in \\{-1, +1\\}$, we have the unnormalized log probability:\n",
    "\n",
    "$$\\log \\tilde { p } ( \\mathbf { y } ) = \\sum _ { s \\sim t } w _ { s t } y _ { s } y _ { t } + \\sum _ { s } b _ { s } y _ { s } = \\frac { 1 } { 2 } \\mathbf { y } ^ { T } \\mathbf { W } \\mathbf { y } + \\mathbf { b } ^ { T } \\mathbf { y }$$\n",
    "\n",
    "where $\\theta = (W,b)$, the normalizing term: $Z$ requires summing over all $2^D$ bit vectors, which is NP-hard in general\n",
    "\n",
    "### Hopfield network:\n",
    "+ used for associative moemory or pattern completion, \n",
    "+ we have iterative conditional modes (ICM) for inference. set each node to its most likely (lowest energy) state, given all its neighbors, recurrent neural network:\n",
    "\n",
    "$$p \\left( y _ { s } = 1 | \\mathbf { y } _ { - s } , \\boldsymbol { \\theta } \\right) = \\operatorname { sigm } \\left( \\mathbf { w } _ { s,: } ^ { T } \\boldsymbol { y }_{-s}  + b _ { s } \\right)$$\n",
    "\n",
    "+ Boltzmann machine: generalizes the Hopfield/Ising model by including some hidden nodes, which makes the model representationally more powerful\n",
    "\n",
    "### Potts model:\n",
    "Generalize the Ising model to multiple discrete states $y _ { t } \\in \\{ 1,2 , \\dots , K \\}$.\n",
    "\n",
    "+ Potential function:\n",
    "$$\\psi _ { s t } \\left( y _ { s } , y _ { t } \\right) = \\left( \\begin{array} { c c c } { e ^ { J } } & { 0 } & { 0 } \\\\ { 0 } & { e ^ { J } } & { 0 } \\\\ { 0 } & { 0 } & { e ^ { J } } \\end{array} \\right)$$\n",
    "\n",
    "+ Used as a prior for image segmentation: neighboring pixels are likely to have the same discrete label and hence belong to the same segment.\n",
    "$$p ( \\mathbf { y } , \\mathbf { x } | \\boldsymbol { \\theta } ) = p ( \\mathbf { y } | J ) \\prod _ { t } p \\left( x _ { t } | y _ { t } , \\boldsymbol { \\theta } \\right) = \\left[ \\frac { 1 } { Z ( J ) } \\prod _ { s \\sim t } \\psi \\left( y _ { s } , y _ { t } ; J \\right) \\right] \\prod _ { t } p \\left( x _ { t } | y _ { t } , \\boldsymbol { \\theta } \\right)$$\n",
    "\n",
    "+ Chain graph: combination of an undirected and directed graph, local evidence: $p(x_t|y_t, \\theta)$\n",
    "\n",
    "![](../images/19.Potts.png)\n",
    "\n",
    "### Gaussian MRFs:\n",
    "\n",
    "$$\\begin{aligned} p ( \\mathbf { y } | \\boldsymbol { \\theta } ) & \\propto \\prod _ { s \\sim t } \\psi _ { s t } \\left( y _ { s } , y _ { t } \\right) \\prod _ { t } \\psi _ { t } \\left( y _ { t } \\right) \\\\ \\psi _ { s t } \\left( y _ { s } , y _ { t } \\right) & = \\exp \\left( - \\frac { 1 } { 2 } y _ { s } \\Lambda _ { s t } y _ { t } \\right) \\\\ \\psi _ { t } \\left( y _ { t } \\right) & = \\exp \\left( - \\frac { 1 } { 2 } \\Lambda _ { t t } y _ { t } ^ { 2 } + \\eta _ { t } y _ { t } \\right) \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "+ Training MaxEnt models using gradient methods:\n",
    "    $$p ( \\mathbf { y } | \\boldsymbol { \\theta } ) = \\frac { 1 } { Z ( \\boldsymbol { \\theta } ) } \\exp \\left( \\sum _ { c } \\boldsymbol { \\theta } _ { c } ^ { T } \\boldsymbol { \\phi } _ { c } ( \\mathbf { y } ) \\right)$$\n",
    "\n",
    "    Scaled log-likelihood: \n",
    "    $$\\ell ( \\boldsymbol { \\theta } ) \\triangleq \\frac { 1 } { N } \\sum _ { i } \\log p \\left( \\mathbf { y } _ { i } | \\boldsymbol { \\theta } \\right) = \\frac { 1 } { N } \\sum _ { i } \\left[ \\sum _ { c } \\boldsymbol { \\theta } _ { c } ^ { T } \\boldsymbol { \\phi } _ { c } \\left( \\mathbf { y } _ { i } \\right) - \\log Z ( \\boldsymbol { \\theta } ) \\right]$$\n",
    "\n",
    "    so we have $$\\frac { \\partial \\ell } { \\partial \\boldsymbol { \\theta } _ { c } } = \\left[ \\frac { 1 } { N } \\sum _ { i } \\boldsymbol { \\phi } _ { c } \\left( \\mathbf { y } _ { i } \\right) \\right] - \\mathbb { E } \\left[ \\boldsymbol { \\phi } _ { c } ( \\mathbf { y } ) \\right] = \\mathbb { E } _ { p _ { \\mathrm { emp } } } \\left[ \\boldsymbol { \\phi } _ { c } ( \\mathbf { y } ) \\right] - \\mathbb { E } _ { p ( \\cdot | \\boldsymbol { \\theta } ) } \\left[ \\boldsymbol { \\phi } _ { c } ( \\mathbf { y } ) \\right]$$\n",
    "\n",
    "    In the first term, we fix $y$ to its observed values, or **clamped term**. In the second term, $y$ is free, **unclamped term** or constrastive term. Note that **computing the unclamped term requires inference in the model, and this must be done once per gradient step**. That makes UGM training much slower than DGM training. The gradient of the log likelihood can be rewritten as the expected feature vector according to the empirical distribution minus the model’s expectation of the feature vector:\n",
    "\n",
    "    So we have: \n",
    "    $$\\mathbb { E } _ { p _ { \\text { omp } } } \\left[ \\phi _ { c } ( \\mathbf { y } ) \\right] = \\mathbb { E } _ { p ( \\cdot | \\theta ) } \\left[ \\phi _ { c } ( \\mathbf { y } ) \\right]$$\n",
    "\n",
    "    ==> Moment matching\n",
    "\n",
    "+ Training partially observed Maxent models (with hidden variables)\n",
    "    $$p ( \\mathbf { y } , \\mathbf { h } | \\boldsymbol { \\theta } ) = \\frac { 1 } { Z ( \\boldsymbol { \\theta } ) } \\exp \\left( \\sum _ { c } \\boldsymbol { \\theta } _ { c } ^ { T } \\boldsymbol { \\phi } _ { c } ( \\mathbf { h } , \\mathbf { y } ) \\right)$$\n",
    "    \n",
    "    Log likelihood:\n",
    "    $$\\ell ( \\boldsymbol { \\theta } ) = \\frac { 1 } { N } \\sum _ { i } \\log \\left( \\sum _ { \\mathbf { h } _ { i } } p \\left( \\mathbf { y } _ { i } , \\mathbf { h } _ { i } | \\boldsymbol { \\theta } \\right) \\right) = \\frac { 1 } { N } \\sum _ { i } \\log \\left( \\frac { 1 } { Z ( \\boldsymbol { \\theta } ) } \\sum _ { \\mathbf { h } _ { i } } \\tilde { p } \\left( \\mathbf { y } _ { i } , \\mathbf { h } _ { i } | \\boldsymbol { \\theta } \\right) \\right)$$\n",
    "    \n",
    "    where unnormalized distribution: \n",
    "    $$\\tilde { p } ( \\mathbf { y } , \\mathbf { h } | \\boldsymbol { \\theta } ) \\triangleq \\exp \\left( \\sum _ { c } \\boldsymbol { \\theta } _ { c } ^ { T } \\boldsymbol { \\phi } _ { c } ( \\mathbf { h } , \\mathbf { y } ) \\right)$$\n",
    "    \n",
    "    Therefore, averaging over $h$\n",
    "    $$\\frac { \\partial \\ell } { \\partial \\boldsymbol { \\theta } _ { c } } = \\frac { 1 } { N } \\sum _ { i } \\left\\{ \\mathbb { E } \\left[ \\boldsymbol { \\phi } _ { c } \\left( \\mathbf { h } , \\mathbf { y } _ { i } \\right) | \\boldsymbol { \\theta } \\right] - \\mathbb { E } \\left[ \\boldsymbol { \\phi } _ { c } ( \\mathbf { h } , \\mathbf { y } ) | \\boldsymbol { \\theta } \\right] \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Field (CRFs)\n",
    "\n",
    "### Introduction\n",
    "a.k.a Discriminative random field, a version of MRF where all clique potentials are conditioned on input features. It is structured output extension of logistic regression.\n",
    "\n",
    "$$p ( \\mathbf { y } | \\mathbf { x } , \\mathbf { w } ) = \\frac { 1 } { Z ( \\mathbf { x } , \\mathbf { w } ) } \\prod _ { c } \\psi _ { c } \\left( \\mathbf { y } _ { c } | \\mathbf { x } , \\mathbf { w } \\right)$$\n",
    "\n",
    "where: $\\psi _ { c } \\left( \\mathbf { y } _ { c } | \\mathbf { x } , \\mathbf { w } \\right) = \\exp \\left( \\mathbf { w } _ { c } ^ { T } \\boldsymbol { \\phi } \\left( \\mathbf { x } , \\mathbf { y } _ { c } \\right) \\right)$, $\\boldsymbol { \\phi } \\left( \\mathbf { x } , \\mathbf { y } _ { c } \\right)$ is a feature vector derived from the global inputss $x$ and local set of labels $y_c$. \n",
    "\n",
    "+ Advantage of CRF over MRF is analogous to the advantage of a discriminative classifer over a generative classifier:\n",
    "    + We don't waste resources modeling things taht we always observe. Instead, we focus our attention on modeling that we care about, namely the distribution of labels given the data\n",
    "    + make potentials of the model be data-dependent\n",
    "    \n",
    "+ Disadvantage: require labeled training data => slower to train.\n",
    "\n",
    "### Chain-structured CRFs, MEMMs:\n",
    "![](../images/19.CRF.png)\n",
    "+ Hidden Markov Models (HMMs):\n",
    "$$p ( \\mathbf { x } , \\mathbf { y } | \\mathbf { w } ) = \\prod _ { t = 1 } ^ { T } p \\left( y _ { t } | y _ { t - 1 } , \\mathbf { w } \\right) p \\left( \\mathbf { x } _ { t } | y _ { t } , \\mathbf { w } \\right)$$\n",
    "\n",
    "+ Maximum Entropy Markov Models (MEMMs):\n",
    "$$p ( \\mathbf { y } | \\mathbf { x } , \\mathbf { w } ) = \\prod _ { t } p \\left( y _ { t } | y _ { t - 1 } , \\mathbf { x } , \\mathbf { w } \\right)$$\n",
    "\n",
    "+ Chain-structured CRFs:\n",
    "$$p ( \\mathbf { y } | \\mathbf { x } , \\mathbf { w } ) = \\frac { 1 } { Z ( \\mathbf { x } , \\mathbf { w } ) } \\prod _ { t = 1 } ^ { T } \\psi \\left( y _ { t } | \\mathbf { x } , \\mathbf { w } \\right) \\prod _ { t = 1 } ^ { T - 1 } \\psi \\left( y _ { t } , y _ { t + 1 } | \\mathbf { x } , \\mathbf { w } \\right)$$\n",
    "\n",
    "### Training:\n",
    "+ log-likelihood:\n",
    "$$\\ell ( \\mathbf { w } ) \\triangleq \\frac { 1 } { N } \\sum _ { i } \\log p \\left( \\mathbf { y } _ { i } | \\mathbf { x } _ { i } , \\mathbf { w } \\right) = \\frac { 1 } { N } \\sum _ { i } \\left[ \\sum _ { c } \\mathbf { w } _ { c } ^ { T } \\boldsymbol { \\phi } _ { c } \\left( \\mathbf { y } _ { i } , \\mathbf { x } _ { i } \\right) - \\log Z \\left( \\mathbf { w } , \\mathbf { x } _ { i } \\right) \\right]$$\n",
    "\n",
    "+ Gradient:\n",
    "$$\\begin{aligned} \\frac { \\partial \\ell } { \\partial \\mathbf { w } _ { c } } & = \\frac { 1 } { N } \\sum _ { i } \\left[ \\boldsymbol { \\phi } _ { c } \\left( \\mathbf { y } _ { i } , \\mathbf { x } _ { i } \\right) - \\frac { \\partial } { \\partial \\mathbf { w } _ { c } } \\log Z \\left( \\mathbf { w } , \\mathbf { x } _ { i } \\right) \\right] \\\\ & = \\frac { 1 } { N } \\sum _ { i } \\left[ \\boldsymbol { \\phi } _ { c } \\left( \\mathbf { y } _ { i } , \\mathbf { x } _ { i } \\right) - \\mathbb { E } \\left[ \\boldsymbol { \\phi } _ { c } \\left( \\mathbf { y } , \\mathbf { x } _ { i } \\right) \\right] \\right] \\end{aligned}$$\n",
    "\n",
    "+ Prevent overfitting by using Gaussian prior:\n",
    "$$\\ell ^ { \\prime } ( \\mathbf { w } ) \\triangleq \\frac { 1 } { N } \\sum _ { i } \\log p \\left( \\mathbf { y } _ { i } | \\mathbf { x } _ { i } , \\mathbf { w } \\right) - \\lambda \\| \\mathbf { w } \\| _ { 2 } ^ { 2 }$$\n",
    "\n",
    "or $l_1$ for the edge weights $w_e$ to learn a sparse graph structure and $l_2$ for the node weights $w_n$: \n",
    "$$\\ell ^ { \\prime } ( \\mathbf { w } ) \\triangleq \\frac { 1 } { N } \\sum _ { i } \\log p \\left( \\mathbf { y } _ { i } | \\mathbf { x } _ { i } , \\mathbf { w } \\right) - \\lambda _ { 1 } \\left\\| \\mathbf { w } _ { e } \\right\\| _ { 1 } - \\lambda _ { 2 } \\left\\| \\mathbf { w } _ { n } \\right\\| _ { 2 } ^ { 2 }$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
