{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Clustering is the process of grouping similar objects together. \n",
    "\n",
    "There are two kinds of inputs:\n",
    "+ similarity-based clustering: input is $N \\times N$ dissimilarity matrix or distance matrix $\\mathbf{D}$: easy inclusion of domain-specific similarity or kernel functions.\n",
    "+ featured-based clustering: input is $N \\times D$ feature matrix or design matrix $X$: applicable to \"raw\" potentially noisy data.\n",
    "\n",
    "There are two types of output:\n",
    "+ Flat clustering (partitional clustering): we partition the objects into disjoint sets: faster to create ($O(ND) \\text{ vs } O(N^2\\log N)$))\n",
    "+ Hierarchical clustering: we create nested tree of partitions: often more useful and do not require the specification of $K$\n",
    "\n",
    "### Measuring (dis)similarity\n",
    "A dissimilarity matrix $\\mathbf{D}$ is a matrix where $d_{i,i}=0$ and $d_{i,j} \\geq 0$ is a measure of distance between objects $i$ and $j$. If we have a similarity matrix $\\mathbf{S}$, we can convert it to a dissimilarity matrix by apply any monotonically decreasing function: $\\mathbf { D } = \\max ( \\mathbf { S } ) - \\mathbf { S }$\n",
    "\n",
    "$$\\Delta \\left( \\mathbf { x } _ { i } , \\mathbf { x } _ { i ^ { \\prime } } \\right) = \\sum _ { j = 1 } ^ { D } \\Delta _ { j } \\left( x _ { i j } , x _ { i ^ { \\prime } j } \\right)$$\n",
    "\n",
    "+ Squared (Euclidean) distance (emphasize large distances):\n",
    "    $$\\Delta _ { j } \\left( x _ { i j } , x _ { i ^ { \\prime } j } \\right) = \\left( x _ { i j } - x _ { i ^ { \\prime } j } \\right) ^ { 2 }$$\n",
    "    \n",
    "+ L1 distance (city block distance, manhattan distance):\n",
    "    $$\\Delta _ { j } \\left( x _ { i j } , x _ { i ^ { \\prime } j } \\right) = \\left| x _ { i j } - x _ { i ^ { \\prime } j } \\right|$$\n",
    "    \n",
    "+ Dot product distance:\n",
    "    $$\\operatorname { corr } \\left[ \\mathbf { x } _ { i } , \\mathbf { x } _ { i ^ { \\prime } } \\right] = \\sum _ { j } x _ { i j } x _ { i ^ { \\prime } j }$$\n",
    "    \n",
    "+ For ordinal variables: such as {low, medium, high}: encode the values as real-valued numbers: 1/3, 2/3, 3/3 if there are 3 possible values\n",
    "\n",
    "+ For categorical variables, such as {red, green, blue}, we usually assign a distance of 1 if the features are different and distance of 0 otherwise (hamming distance):\n",
    "    $$\\Delta \\left( \\mathbf { x } _ { i } , \\mathbf { x } _ { i } \\right) = \\sum _ { j = 1 } ^ { D } \\mathbb { I } \\left( x _ { i j } \\neq x _ { i ^ { \\prime } j } \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-medioids or K-centers:\n",
    "Similar to k-means but instead of representing each cluster's centeroid by the mean of all data vectors assigned to this cluster, we make each centroid be one of the data vectors themselves. When update the centroids, we look at each object that belongs to the cluster, and measure the sum of its distances to all others in the same cluster;, we then pick the one which has the smallest such sum\n",
    "\n",
    "![](../images/25.K-medoids.png)\n",
    "\n",
    "where $d \\left( i , i ^ { \\prime } \\right) \\triangleq \\left\\| \\mathbf { x } _ { i } - \\mathbf { x } _ { i ^ { \\prime } } \\prime \\right\\| _ { 2 } ^ { 2 }$. It can be suffered from local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity Propagation\n",
    "Main idea: each data point must choose another data point as its exemplar or centeroid; some data points will choose themselves as centeroids, and this will automatically determine the number of cluster. Let $c _ { i } \\in \\{ 1 , \\ldots , N \\}$ represents the centroid for datapoint $i$.\n",
    "\n",
    "The objective function to be maximize: \n",
    "\n",
    "$$S ( \\mathbf { c } ) = \\sum _ { i = 1 } ^ { N } s \\left( i , c _ { i } \\right) + \\sum _ { k = 1 } ^ { N } \\delta _ { k } ( \\mathbf { c } )$$\n",
    "\n",
    "The first term measures the similarity of each point to its centroid. The second term is a penalty term that is $-\\infty$ if some data point $i$ has chosen $k$ as its examplar, but $k$ has not chosen itself as an examplar (i.e. we do not have $c_k = k$):\n",
    "\n",
    "$$\\delta _ { k } ( \\mathbf { c } ) = \\left\\{ \\begin{array} { l l } { - \\infty } & { \\text { if } c _ { k } \\neq k \\text { but } \\exists i : c _ { i } = k } \\\\ { 0 } & { \\text { otherwise } } \\end{array} \\right.$$\n",
    "\n",
    "Factor graphs for affinity propagation:\n",
    "![](../images/25.AP.png)\n",
    "\n",
    "We use max-product loopy belief propagation. \n",
    "+ Each variable nodes $c_i$ sends a message to each factor node $\\delta_k$: $r_{i \\rightarrow k}$: responsibility: measure of how much $i$ thinks $k$ would make a good exemplar, compared to all other examplars $i$ has looked at.\n",
    "    $$r ( i , k ) \\leftarrow s ( i , k ) - \\max _ { k ^ { \\prime } s . t , k \\neq k } \\left\\{ a \\left( i , k ^ { \\prime } \\right) + s \\left( i , k ^ { \\prime } \\right) \\right\\}$$\n",
    "+ Each factor node $\\delta_k$ sends a message to each variable node $c_i$: $a_{i \\leftarrow k}$: availability. This is measure of how strongly $k$ believes it should be an examplar for $k$ based on all other data points $k$ has looked at:\n",
    "    $$a ( i , k ) \\leftarrow \\min \\left\\{ 0 , r ( k , k ) + \\sum _ { i s . t , i \\notin \\{ i , k \\} } \\max \\{ 0 , r ( i , k ) \\} \\right\\}$$\n",
    "    $$a ( k , k ) \\leftarrow \\sum _ { i \\text { s.t. } i \\neq k } \\max \\{ 0 , r ( i , k ) \\}$$\n",
    "+ The number of clusters can be controlled by scaling the diagonal terms $S(i,i)$ which reflect how much each data point wants to be an examplr. usually set to be the median of all the pairwise similarities.\n",
    "\n",
    "![](../images/25.AP2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral clustering\n",
    "the idea is we create a weighted undirected graph $\\mathbf{W}$ from the similarity matrix $S$, typically by using the nearest neighbors of each point (sparse). We want to find a partition into $K$ clusters: $A_1, \\ldots, A_K$. \n",
    "\n",
    "Graph cuts: \n",
    "$$\\operatorname { cut } \\left( A _ { 1 } , \\ldots , A _ { K } \\right) \\triangleq \\frac { 1 } { 2 } \\sum _ { k = 1 } ^ { K } W \\left( A _ { k } , \\overline { A } _ { k } \\right)$$\n",
    "\n",
    "where $\\overline { A } _ { k } = V \\backslash A _ { k }$ is the complement of $A_k$ and $W ( A , B ) \\triangleq \\sum _ { i \\in A , j \\in B } w _ { i j }$. \n",
    "\n",
    "Normalized cut: \n",
    "$$\\operatorname { Ncut } \\left( A _ { 1 } , \\ldots , A _ { K } \\right) \\triangleq \\frac { 1 } { 2 } \\sum _ { k = 1 } ^ { K } \\frac { \\operatorname { cut } \\left( A _ { k } , \\overline { A } _ { k } \\right) } { \\operatorname { vol } \\left( A _ { k } \\right) }$$\n",
    "\n",
    "where $\\operatorname { vol } ( A ) \\triangleq \\sum _ { i \\in A } d _ { i } , \\text { and } d _ { i } = \\sum _ { j = 1 } ^ { N } w _ { i j }$ is the weighted degree of node $i$. This splits the graph into $K$ clusters such that nodes within each cluster are similar to each other, but are different to nodes in other clusters.\n",
    "\n",
    "+ Formulate Ncut problem in terms of searching for binary vectors $\\mathbf{c}_i \\in \\{0, 1\\}^N$ where $c_{ik} = 1$ if point $i$ belongs to cluster $k$ that minimize the objective. We can solve by using affinity propagation. \n",
    "+ We can also relax the constraints that $\\mathbf{c}_i$ to be binary and allow them to be real-valued => Eigenvector problem: spectral clustering. Technique of performing eigenalysis of graphs is called spectral graph theory.\n",
    "\n",
    "### Graph Laplacian:\n",
    "Let $\\mathbf{W}$ be a symmetric weight matrix for a graph, $w _ { i j } = w _ { j i } \\geq 0$, $\\mathbf { D } = \\operatorname { diag } \\left( d _ { i } \\right)$ be diagonal matrix containing the wighted degree of each node (sum of each rows). Graph Laplacian:\n",
    "\n",
    "$$\\mathbf { L } \\triangleq \\mathbf { D } - \\mathbf { W }$$\n",
    "\n",
    "Properties:\n",
    "+ Each row sums to zero, we have that $\\mathbf{1}$ is an eigenvector with eigenvalue 0.\n",
    "+ The matrix is symmetric and positive semi-definite. => $\\mathbf{L}$ has $N$ non-negative, real eigenvalues $0 \\leq \\lambda _ { 1 } \\leq \\lambda _ { 2 } \\leq \\ldots \\leq \\lambda _ { N }$\n",
    "\n",
    "Theorem: The set of eigenvectors of $\\mathbf{L}$  with eigenvalue 0 is spanned by the indicator vectors $\\mathbf { 1 } _ { A _ { 1 } } , \\dots , \\mathbf { 1 } _ { A _ { K } }$ , where $A_k$ are the $K$ connected components of the graph.\n",
    "\n",
    "![](../images/25.SP1.png)\n",
    "\n",
    "### Normalized Graph Laplacian\n",
    "because some nodes are more highly connected than others:\n",
    "+ Shi and Malik 2000:\n",
    "    $$\\mathbf { L } _ { r w } \\triangleq \\mathbf { D } ^ { - 1 } \\mathbf { L } = \\mathbf { I } - \\mathbf { D } ^ { - 1 } \\mathbf { W }$$\n",
    "+ Ng et al. 2001:\n",
    "    $$\\mathbf { L } _ { s y m } \\triangleq \\mathbf { D } ^ { - \\frac { 1 } { 2 } } \\mathbf { L D } ^ { - \\frac { 1 } { 2 } } = \\mathbf { I } - \\mathbf { D } ^ { - \\frac { 1 } { 2 } } \\mathbf { W } \\mathbf { D } ^ { - \\frac { 1 } { 2 } }$$\n",
    "    \n",
    "![](../images/25.SP2.png) \n",
    "![](../images/25.SP3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "+ bottom-up or agglomerative clustering\n",
    "+ top-down or divisive clustering\n",
    "\n",
    "inputs: dissimilarity between the objects. They are both just heuristics which do not optimize any well-defined objective function. \n",
    "\n",
    "### Agglomerative clustering\n",
    "Agglomerative clustering starts with N groups, each initially containing one object, and then at each step it merges the two most similar groups until there is a single group, containing all the data.\n",
    "\n",
    "![](../images/25.AC.png)\n",
    "\n",
    "Since picking the two most similar clusters to merge takes $O(N^2)$ time, and there are $O(N)$ steps in the algorithm, the total running time is $O(N^3)$. However, by using a priority queue, this can be reduced to $O(N^2 \\log N)$\n",
    "\n",
    "Merging process can be represented by a binary tree called dendrogram. Three dissimilarities between two groups:\n",
    "\n",
    "![](../images/25.Dis.png)\n",
    "\n",
    "### Divisive clustering:\n",
    "Divisive clustering starts with all the data in a single cluster, and then recursively divides each cluster into two daughter clusters, in a top-down fashion. One approach is pick the cluster with the largest diameter, and split it in two using the K-means or K-medoids algorithm with $K = 2$. This is called the bisecting K-means algorithm. We can repeat this until we have any desired number of clusters.\n",
    "\n",
    "It is less popular than agglomerative clustering, but it has two advantages:\n",
    "+ First, it can be faster, since if we only split for a constant number of levels, it takes just $O(N)$ time. \n",
    "+ Second, the splitting decisions are made in the context of seeing all the data, whereas bottom-up methods make myopic merge decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
