{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive basis function models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We target to learn useful features $\\phi(x)$ directly from the data without using kernel function. \n",
    "Adaptive basis-function model (ABM):\n",
    "\n",
    "$$f ( \\mathbf { x } ) = w _ { 0 } + \\sum _ { m = 1 } ^ { M } w _ { m } \\phi _ { m } ( \\mathbf { x } )$$\n",
    "\n",
    "where $\\phi_m(x)$ is the $m$'th basis function, which is learned from data. We can write $\\phi _ { m } ( \\mathbf { x } ) = \\phi \\left( \\mathbf { x } ; \\mathbf { v } _ { m } \\right)$ where $v_m$ are the parameters of the basis function itself. We will use $\\boldsymbol { \\theta } = \\left( w _ { 0 } , \\mathbf { w } _ { 1 : M } , \\left\\{ \\mathbf { v } _ { m } \\right\\} _ { m = 1 } ^ { M } \\right)$ to denote the entire parameter set. The resulting model is not linear -in-the-parameters anymore, so will will only be able to compute the locally optimal MLE or MAP estimate of $\\theta$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and regression trees (CART)\n",
    "or decision trees.\n",
    "\n",
    "### Intro\n",
    "model:\n",
    "$$f ( \\mathbf { x } ) = \\mathbb { E } [ y | \\mathbf { x } ] = \\sum _ { m = 1 } ^ { M } w _ { m } \\mathbb { I } \\left( \\mathbf { x } \\in R _ { m } \\right) = \\sum _ { m = 1 } ^ { M } w _ { m } \\phi \\left( \\mathbf { x } ; \\mathbf { v } _ { m } \\right)$$\n",
    "\n",
    "where $R_m$ is the $m$'th region, $w_m$ is the mean response in this region, and $v_m$ encodes the choice of variable to split on (the threshold value, on the path from the root to the $m$'th leaf. \n",
    "\n",
    "It is adaptive basis functions: \n",
    "+ basis function: define the regions\n",
    "+ weight: response value in each region\n",
    "\n",
    "![](../images/16.DT.png)\n",
    "\n",
    "### Growing a tree: \n",
    "(used in C4.5 and ID3). The split function chooses the best feature, and the best value for that feature, as follows:\n",
    "\n",
    "$$\\left( j ^ { * } , t ^ { * } \\right) = \\arg \\min _ { j \\in \\{ 1 , \\ldots , D \\} } \\min _ { t \\in \\mathcal { T } _ { j } } \\operatorname { cost } \\left( \\left\\{ \\mathbf { x } _ { i } , y _ { i } : x _ { i j } \\leq t \\right\\} \\right) + \\operatorname { cost } \\left( \\left\\{ \\mathbf { x } _ { i } , y _ { i } : x _ { i j } > t \\right\\} \\right)$$\n",
    "\n",
    "![](../images/16.DT_algo.png)\n",
    "\n",
    "where the worthSplitting function: \n",
    "$$\\Delta \\triangleq \\operatorname { cost } ( \\mathcal { D } ) - \\left( \\frac { \\left| \\mathcal { D } _ { L } \\right| } { | \\mathcal { D } | } \\operatorname { cost } \\left( \\mathcal { D } _ { L } \\right) + \\frac { \\left| \\mathcal { D } _ { R } \\right| } { | \\mathcal { D } | } \\operatorname { cost } \\left( \\mathcal { D } _ { R } \\right) \\right)$$\n",
    "\n",
    "+ Regression cost:\n",
    "$$\\operatorname { cost } ( \\mathcal { D } ) = \\sum _ { i \\in \\mathcal { D } } \\left( y _ { i } - \\overline { y } \\right) ^ { 2 }$$\n",
    "where $\\overline { y } = \\frac { 1 } { | \\mathcal { D } | } \\sum _ { i \\in \\mathcal { D } } y _ { i }$ is the mean of the response variable in the specified set of data.\n",
    "\n",
    "+ Classification cost:\n",
    "    First, we fit a multinoulli (categorical) model to the data in the leaf satisfying the test: $X_j < t$ by: $$\\hat { \\pi } _ { c } = \\frac { 1 } { | \\mathcal { D } | } \\sum _ { i \\in \\mathcal { D } } \\mathbb { I } \\left( y _ { i } = c \\right)$$\n",
    "\n",
    "    where $\\mathcal{D}$ is the data in the leaf. There are several common error measures for evaluating a proposed partition:\n",
    "\n",
    "    + Misclassification rate: most probable class label: $\\hat { y } _ { c } = \\operatorname { argmax } _ { c } \\hat { \\pi } _ { c }$. The corresponding error rate: $$\\frac { 1 } { | \\mathcal { D } | } \\sum _ { i \\in \\mathcal { D } } \\mathbb { I } \\left( y _ { i } \\neq \\hat { y } \\right) = 1 - \\hat { \\pi } _ { \\hat { y } }$$\n",
    "    \n",
    "    + Entropy (deviance, information gain): $$\\mathbb { H } ( \\hat { \\boldsymbol { \\pi } } ) = - \\sum _ { c = 1 } ^ { C } \\hat { \\pi } _ { c } \\log \\hat { \\pi } _ { c }$$\n",
    "    \n",
    "    + Gini index: $$\\sum _ { c = 1 } ^ { C } \\hat { \\pi } _ { c } \\left( 1 - \\hat { \\pi } _ { c } \\right) = \\sum _ { c } \\hat { \\pi } _ { c } - \\sum _ { c } \\hat { \\pi } _ { c } ^ { 2 } = 1 - \\sum _ { c } \\hat { \\pi } _ { c } ^ { 2 }$$\n",
    "    \n",
    "### Pros and cons of trees:\n",
    "+ Pros:\n",
    "    + easy to interpret\n",
    "    + easily handle mixed discrete and continuous inputs\n",
    "    + insensitive to monotone transofrmations of the inputs (split points are based on ranking the data points)\n",
    "    + peform automatic variable selection, \n",
    "    + robust to outliers\n",
    "    + scale well to large dataset\n",
    "    + can be modified to handle missing inputs\n",
    "+ Cons:\n",
    "    + do not predict very accurately compared to other kinds of model.\n",
    "    + unstable: small changes to the input data can have large effects on the structure of the tree, due to the hierarchical nature of the tree-growing process.\n",
    "    \n",
    "### Random forests:\n",
    "To reduce the variance of an estimate is to average together many estimates. \n",
    "\n",
    "+ Bagging: we can train $M$ different trees on different subsets of the data, chosen randomly with replacement, and then compute the ensemble: \n",
    "\n",
    "    $$f ( \\mathbf { x } ) = \\sum _ { m = 1 } ^ { M } \\frac { 1 } { M } f _ { m } ( \\mathbf { x } )$$\n",
    "\n",
    "    where $f_m$ is the $m$'th tree.\n",
    "\n",
    "+ Random forests: tries to decorrelate the base learners by learning trees based on a randomly chosen subset of input **features**, as well as a randomly chosen subset of **data** cases. Such models often have very good predictive accuracy: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "### Intro:\n",
    "Boosting is a greedy algorithm for fitting adaptive basis-function models of the form: \n",
    "$$f ( \\mathbf { x } ) = w _ { 0 } + \\sum _ { m = 1 } ^ { M } w _ { m } \\phi _ { m } ( \\mathbf { x } )$$\n",
    "\n",
    "where $\\phi_m$ are generated by an algorithm called weak learner or base learner. The algorithm works by applying th weak learner sequentially to **weighted versions** of the data, where more weight is given to examples that were misclassified by earlier rounds. The weak learneer can by any classification or regression algorithm, but is common to use a CART model. Boosting is very resistant to overfitting.\n",
    "\n",
    "### Forward stagewise additive modeling:\n",
    "The goal of boosting is to solve the following optimization problem:\n",
    "$$\\min _ { f } \\sum _ { i = 1 } ^ { N } L \\left( y _ { i } , f \\left( \\mathbf { x } _ { i } \\right) \\right)$$\n",
    "\n",
    "where $L(y, \\hat{y})$ is some loss functions, and $f$ is assumed to be an ABM model.\n",
    "\n",
    "Some common choices for the loss function:\n",
    "![](../images/16.boost.png)\n",
    "\n",
    "Process:\n",
    "+ First, we initialize:\n",
    "$$f _ { 0 } ( \\mathbf { x } ) = \\arg \\min _ { \\gamma } \\sum _ { i = 1 } ^ { N } L \\left( y _ { i } , f \\left( \\mathbf { x } _ { i } ; \\gamma \\right) \\right)$$\n",
    "\n",
    "    for example: \n",
    "    + for squared error loss: $f _ { 0 } ( \\mathbf { x } ) = \\overline { y }$\n",
    "    + for log-loss or exponential loss: $f _ { 0 } ( \\mathbf { x } ) = \\frac { 1 } { 2 } \\log \\frac { \\hat { n } } { 1 - \\tilde { \\pi } } , \\text { where } \\hat { \\pi } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\mathbb { I } \\left( y _ { i } = 1 \\right)$\n",
    "\n",
    "+ We iteratively compute:\n",
    "$$\\left( \\beta _ { m } , \\gamma _ { m } \\right) = \\underset { \\beta , \\gamma } { \\operatorname { argmin } } \\sum _ { i = 1 } ^ { N } L \\left( y _ { i } , f _ { m - 1 } \\left( \\mathbf { x } _ { i } \\right) + \\beta \\phi \\left( \\mathbf { x } _ { i } ; \\gamma \\right) \\right)$$\n",
    "    and set: $$f _ { m } ( \\mathbf { x } ) = f _ { m - 1 } ( \\mathbf { x } ) + \\nu\\beta _ { m } \\phi \\left( \\mathbf { x } ; \\gamma _ { m } \\right)$$\n",
    "    \n",
    "    where $0 < \\nu < 1$ is learning rate, common to use $\\nu = 0.1$\n",
    "    \n",
    "+ the keypoint: we do not go back and adjust earlier parameter, we just add new function to the list of current ones\n",
    "\n",
    "### L2boosting:\n",
    "Squared error loss:\n",
    "$$L \\left( y _ { i } , f _ { m - 1 } \\left( \\mathbf { x } _ { i } \\right) + \\beta \\phi \\left( \\mathbf { x } _ { i } ; \\gamma \\right) \\right) = \\left( r _ { i m } - \\phi \\left( \\mathbf { x } _ { i } ; \\gamma \\right) \\right) ^ { 2 }$$\n",
    "\n",
    "where $r _ { i m } \\triangleq y _ { i } - f _ { m - 1 } \\left( \\mathbf { x } _ { i } \\right)$ is the current residual, $\\beta = 1$. We can find new basis function by using the weak learner to predict $r_m$. \n",
    "\n",
    "### Adaboost: \n",
    "Exponential loss, at step $m$:\n",
    "    $$L _ { m } ( \\phi ) = \\sum _ { i = 1 } ^ { N } \\exp \\left[ - \\tilde { y } _ { i } \\left( f _ { m - 1 } \\left( \\mathbf { x } _ { i } \\right) + \\beta \\phi \\left( \\mathbf { x } _ { i } \\right) \\right) \\right] = \\sum _ { i = 1 } ^ { N } w _ { i , m } \\exp \\left( - \\beta \\tilde { y } _ { i } \\phi \\left( \\mathbf { x } _ { i } \\right) \\right)$$\n",
    "\n",
    "where $w _ { i , m } \\triangleq \\exp \\left( - \\tilde { y } _ { i } f _ { m - 1 } \\left( \\mathbf { x } _ { i } \\right) \\right)$ is a weight applied to datacase $i$, $\\tilde { y } _ { i } \\in \\{ - 1 , + 1 \\}$\n",
    "\n",
    "![](../images/16.Adaboost.png)\n",
    "\n",
    "### LogitBoost:\n",
    "Expected log-loss:\n",
    "$$L _ { m } ( \\phi ) = \\sum _ { i = 1 } ^ { N } \\log \\left[ 1 + \\exp \\left( - 2 \\tilde { y } _ { i } \\left( f _ { m - 1 } ( \\mathbf { x } ) + \\phi \\left( \\mathbf { x } _ { i } \\right) \\right) \\right) \\right]$$\n",
    "\n",
    "![](../images/16.Logitboost.png)\n",
    "\n",
    "### Gradient Boosting:\n",
    "Work for any loss function: $\\hat { \\mathbf { f } } = \\underset { \\mathbf { f } } { \\operatorname { argmin } } L ( \\mathbf { f } )$\n",
    "\n",
    "where $\\mathbf { f } = \\left( f \\left( \\mathbf { x } _ { 1 } \\right) , \\ldots , f \\left( \\mathbf { x } _ { N } \\right) \\right)$ are the ': parameters'. We solve it stagewise, using gradient descent.\n",
    "\n",
    "At step $m$, let $g_m$ be the gradient of $L(f)$ evaluated at $f=f_{m-1}$:\n",
    "\n",
    "$$g _ { i m } = \\left[ \\frac { \\partial L \\left( y _ { i } , f \\left( \\mathbf { x } _ { i } \\right) \\right) } { \\partial f \\left( \\mathbf { x } _ { i } \\right) } \\right] _ { f = f _ { m - 1 } }$$\n",
    "\n",
    "The we update: $\\mathbf { f } _ { m } = \\mathbf { f } _ { m - 1 } - \\rho _ { m } \\mathbf { g } _ { m }$\n",
    "\n",
    "where $\\rho_m$ is the step length ==> Functional gradient descent.\n",
    "\n",
    "\n",
    "![](../images/16.Gradientboost.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
