{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed Graphical Models (Bayes Nets)\n",
    "\n",
    "## Introduction\n",
    "+ Chain rule:\n",
    "$$p \\left( x _ { 1 : V } \\right) = p \\left( x _ { 1 } \\right) p \\left( x _ { 2 } | x _ { 1 } \\right) p \\left( x _ { 3 } | x _ { 2 } , x _ { 1 } \\right) p \\left( x _ { 4 } | x _ { 1 } , x _ { 2 } , x _ { 3 } \\right) \\ldots p \\left( x _ { V } | x _ { 1 : V - 1 } \\right)$$\n",
    "\n",
    "+ Conditional independence:\n",
    "$$X \\perp Y | Z \\Longleftrightarrow p ( X , Y | Z ) = p ( X | Z ) p ( Y | Z )$$\n",
    "\n",
    "+ First order Markov assumption: the future is independent of the past given the present\n",
    "\n",
    "    $$p \\left( \\mathbf { x } _ { 1 : V } \\right) = p \\left( x _ { 1 } \\right) \\prod _ { t = 1 } ^ { V } p \\left( x _ { t } | x _ { t - 1 } \\right)$$\n",
    "\n",
    "    where $p(x_1)$ is initial distribution and $p \\left( x _ { t } = j | x _ { t - 1 } = i \\right)$ is state transition matrix\n",
    "\n",
    "+ Graphical models (GM) is a way to represent a joint distribution by making Conditional Independence (CI) assumption (independence diagrams)\n",
    "\n",
    "![Direct and Undirect](../images/10.PGM.png)\n",
    "\n",
    "+ Directed Graphical Model (DGM) (Bayesian networks, belief networks, causal networks)\n",
    "\n",
    "+ Ordered Markov property: a node only depends on its immediate parents, not on all predecessors in the ordering: \n",
    "$$p \\left( \\mathbf { x } _ { 1 : V } | G \\right) = \\prod _ { t = 1 } ^ { V } p \\left( x _ { t } | \\mathbf { x } _ { \\mathrm { pa } ( t ) } \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of DGMs:\n",
    "\n",
    "### Naive Bayes classifiers:\n",
    "Assumption: Features are conditionally independent given the class label\n",
    "\n",
    "$$p ( y , \\mathbf { x } ) = p ( y ) \\prod _ { j = 1 } ^ { D } p \\left( x _ { j } | y \\right)$$\n",
    "\n",
    "To capture correlation between the features is to use a graphical model: tree-augmented naive Bayes classifier (TAN)\n",
    "\n",
    "![Naive Bayes](../images/10.NB.png)\n",
    "\n",
    "### Markov and hidden Markov models:\n",
    "+ First-order Markov chain:\n",
    "$$p \\left( \\mathbf { x } _ { 1 : V } \\right) = p \\left( x _ { 1 } \\right) \\prod _ { t = 1 } ^ { V } p \\left( x _ { t } | x _ { t - 1 } \\right)$$\n",
    "\n",
    "+ Second order Markov chain: \n",
    "$$p \\left( \\mathbf { x } _ { 1 : T } \\right) = p \\left( x _ { 1 } , x _ { 2 } \\right) p \\left( x _ { 3 } | x _ { 1 } , x _ { 2 } \\right) p \\left( x _ { 4 } | x _ { 2 } , x _ { 3 } \\right) \\ldots = p \\left( x _ { 1 } , x _ { 2 } \\right) \\prod _ { t = 3 } ^ { T } p \\left( x _ { t } | x _ { t - 1 } , x _ { t - 2 } \\right)$$\n",
    "\n",
    "+ Hidden Markov model: there is an underlying hidden process, that can be modeled by a first-order Markov chain, but that the data is a noisy observation of this process: transition model: $p \\left( z _ { t } | z _ { t - 1 } \\right)$, observation model: $p \\left( \\mathbf { x } _ { t } | z _ { t } \\right)$\n",
    "\n",
    "![HMM](../images/10.HMM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Learning\n",
    "+ Probabilistic inference: the task of estimating unknown quantities (visible variables $x_v$) from known quantities (hidden variables $x_h$) given parameters $\\theta$ are known\n",
    "\n",
    "$$p \\left( \\mathbf { x } _ { h } | \\mathbf { x } _ { v } , \\boldsymbol { \\theta } \\right) = \\frac { p \\left( \\mathbf { x } _ { h } , \\mathbf { x } _ { v } | \\boldsymbol { \\theta } \\right) } { p \\left( \\mathbf { x } _ { v } | \\boldsymbol { \\theta } \\right) } = \\frac { p \\left( \\mathbf { x } _ { h } , \\mathbf { x } _ { v } | \\boldsymbol { \\theta } \\right) } { \\sum _ { \\mathbf { x } _ { h } ^ { \\prime } } p \\left( \\mathbf { x } _ { h } ^ { \\prime } , \\mathbf { x } _ { v } | \\boldsymbol { \\theta } \\right) }$$\n",
    "\n",
    "+ Learning: computing a MAP estimate of the parameters given the data:\n",
    "$$\\hat { \\boldsymbol { \\theta } } = \\underset { \\boldsymbol { \\theta } } { \\operatorname { argmax } } \\sum _ { i = 1 } ^ { N } \\log p \\left( \\mathbf { x } _ { i , v } | \\boldsymbol { \\theta } \\right) + \\log p ( \\boldsymbol { \\theta } )$$\n",
    "\n",
    "    when we have uniform prior $p ( \\boldsymbol { \\theta } ) \\propto 1$, this reduces to the MLE\n",
    "\n",
    "+ Plate notation:\n",
    "$$p ( \\boldsymbol { \\theta } , \\mathcal { D } ) = p ( \\boldsymbol { \\theta } ) \\left[ \\prod _ { i = 1 } ^ { N } p \\left( \\mathbf { x } _ { i } | \\boldsymbol { \\theta } \\right) \\right]$$\n",
    "\n",
    "![Plate](../images/10.Plate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Independence Properties of DGM\n",
    "The heart of any graphical model is a set of conditional independence (CI) assumptions: \n",
    "A is independent of B given C in the graph G: $\\mathbf { x } _ { A } \\perp_G \\mathbf { X } _ { B } \\left| \\mathbf { x } _ { C }\\right.$\n",
    "\n",
    "$I(G)$ is the set of all such CI statements encoded by the graph, $I(p)$ is the set of all CI statements that hold for distribution $p$.\n",
    "\n",
    "$G$ is an I-map (independence map) for $p$ iff $I ( G ) \\subseteq I ( p )$ means: the graph is an I-map if it does not make any assertions of CI that are not true of the distribution.\n",
    "\n",
    "minimal I-map of $p$ if $G$ is an I-map of $p$, and if there is no $G' \\subseteq G$ which is an I-map of $p$.\n",
    "\n",
    "+ d-seperation: an undirected path $P$ is d-separated by a set of nodes $E$ (containing the evidence) iff at least one of the following condition hold:\n",
    "    1. P contains a chain, s → m → t or s ← m ← t, where m ∈ E\n",
    "    2. P contains a tent or fork, $s \\swarrow ^ { m } \\searrow t$, where m ∈ E\n",
    "    3. P contains a collider or v-structure, $s \\searrow m \\swarrow t$, where m is not in E and nor is any descendant of m.\n",
    "\n",
    "a set of nodes $A$ is d-separated from a different set of nodes $B$ given a third observed set $E$ iff each undirected path from every node $a \\in A$ to every node $b \\in B$ is d-separated by $E$: $\\mathbf { x } _ { A } \\perp _ { G } \\mathbf { x } _ { B } \\left| \\mathbf { x } _ { E } \\Longleftrightarrow A \\text { is } d \\text { -separated from } B \\text { given E }\\right.$\n",
    "\n",
    "+ Bayes ball algorithm:\n",
    "![Bayes ball](../images/10.Bayes.png)\n",
    "\n",
    "+ Other Markov properties:\n",
    "\n",
    "++ Directed local property:\n",
    "$$t \\perp \\mathrm { nd } ( t ) \\backslash \\mathrm { pa } ( t ) | \\mathrm { pa } ( t )$$ \n",
    "\n",
    "where nd(t) are non-descendants: \n",
    "$\\mathrm { nd } ( t ) = \\mathcal { V } \\backslash \\{ t \\cup \\operatorname { desc } ( t ) \\}$\n",
    "\n",
    "++ Ordered Markov Proptery:\n",
    "$$t \\perp \\operatorname { pred } ( t ) \\backslash \\mathrm { pa } ( t ) | \\mathrm { pa } ( t )$$\n",
    "\n",
    "![Markov property](../images/10.Markov.png)\n",
    "\n",
    "+ Markov blanket and full conditionals\n",
    "$$\\mathrm { mb } ( t ) \\triangleq \\operatorname { ch } ( t ) \\cup \\mathrm { pa } ( t ) \\cup \\operatorname { copa } ( t )$$\n",
    "\n",
    "$$\\mathrm { mb } ( 5 ) = \\{ 6,7 \\} \\cup \\{ 2,3 \\} \\cup \\{ 4 \\} = \\{ 2,3,4,6,7 \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
