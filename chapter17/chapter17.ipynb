{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Probabilistic models for sequences of observations, $X _ { 1 } , \\dots , X _ { T }$ of arbitrary length $T$. Focus on the case where we have the observation occur at discrete \"time steps\".\n",
    "\n",
    "## Markov models\n",
    "\n",
    "Joint distribution:\n",
    "$$p \\left( X _ { 1 : T } \\right) = p \\left( X _ { 1 } \\right) p \\left( X _ { 2 } | X _ { 1 } \\right) p \\left( X _ { 3 } | X _ { 2 } \\right) \\ldots = p \\left( X _ { 1 } \\right) \\prod _ { t = 2 } ^ { T } p \\left( X _ { t } | X _ { t - 1 } \\right)$$\n",
    "\n",
    "If we assume the transition function $p \\left( X _ { t } | X _ { t - 1 } \\right)$ is independent of time, then the chain is called homogeneous, stationary or time-invariant ==> parameter sharing (tying) ==> allow us to model an arbitrary number of variables using a fixed number of parameters ==> Stochasitc processes.\n",
    "\n",
    "If the observed variables are discrete $X _ { t } \\in \\{ 1 , \\ldots , K \\}$, this is called discrete-state or finite-state Markov chain. \n",
    "\n",
    "### Transition Matrix:\n",
    "the conditional distribution: $p \\left( X _ { t } | X _ { t - 1 } \\right)$ can be written as a $K \\times K$ matrix, a.k.a transition matrix $A$, where $A _ { i j } = p \\left( X _ { t } =j | X _ { t - 1 } = i \\right)$: is the probability of going from state $i$ to state $j$. Each row of the matrix sums to one, $\\sum _ { j } A _ { i j } = 1$, also called stochastic matrix\n",
    "\n",
    "$n$-step transition matrix $A(n)$ is defined as: \n",
    "$$A _ { i j } ( n ) \\triangleq p \\left( X _ { t + n } = j | X _ { t } = i \\right)$$\n",
    "\n",
    "which is the probability of getting from $i$ to $j$ in exactly $n$ steps. \n",
    "\n",
    "The Chapman-Kolmogorov equations: \n",
    "\n",
    "$$A _ { i j } ( m + n ) = \\sum _ { k = 1 } ^ { K } A _ { i k } ( m ) A _ { k j } ( n )$$\n",
    "\n",
    "The probability of getting from $i$ to $j$ in $m + n$ steps is is the probability of getting from $i$ to $k$ in $m$ steps, and the from $k$ to $j$ in $n$ steps, summed up overall $k$. \n",
    "\n",
    "$$\\mathbf { A } ( m + n ) = \\mathbf { A } ( m ) \\mathbf { A } ( n )$$\n",
    "\n",
    "Hence: $$\\mathbf { A } ( n ) = \\mathbf { A } \\mathbf { A } ( n - 1 ) = \\mathbf { A } \\mathbf { A } \\mathbf { A } ( n - 2 ) = \\cdots = \\mathbf { A } ^ { n }$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models\n",
    "HMM consists of a discrete-time, discrete-state markov chain, with hidden states: $z _ { t } \\in \\{ 1 , \\ldots , K \\}$, plus an observation model $p \\left( \\mathbf { x } _ { t } | z _ { t } \\right)$. \n",
    "\n",
    "Joint distribution:\n",
    "$$p \\left( \\mathbf { z } _ { 1 : T } , \\mathbf { x } _ { 1 : T } \\right) = p \\left( \\mathbf { z } _ { 1 : T } \\right) p \\left( \\mathbf { x } _ { 1 : T } | \\mathbf { z } _ { 1 : T } \\right) = \\left[ p \\left( z _ { 1 } \\right) \\prod _ { t = 2 } ^ { T } p \\left( z _ { t } | z _ { t - 1 } \\right) \\right] \\left[ \\prod _ { t = 1 } ^ { T } p \\left( \\mathbf { x } _ { t } | z _ { t } \\right) \\right]$$\n",
    "\n",
    "Observations can be discrete or continuous. \n",
    "+ If they are discrete, observation matrix: $p \\left( \\mathbf { x } _ { t } = l | z _ { t } = k , \\boldsymbol { \\theta } \\right) = B ( k , l )$\n",
    "+ If they are continuous, conditional gaussian: $p \\left( \\mathbf { x } _ { t } | z _ { t } = k , \\boldsymbol { \\theta } \\right) = \\mathcal { N } \\left( \\mathbf { x } _ { t } | \\boldsymbol { \\mu } _ { k } , \\mathbf { \\Sigma } _ { k } \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in HMM\n",
    "Different kinds of inference:\n",
    "+ Filtering: to compute the belief state $p \\left( z _ { t } | \\mathbf { x } _ { 1 : t } \\right)$ online. \n",
    "\n",
    "+ Smoothing: to compute $p \\left( z _ { t } | \\mathbf { x } _ { 1 : T } \\right)$ offline\n",
    "\n",
    "+ Fixed lag smoothing: to compute $p \\left( z _ { t - \\ell } | \\mathbf { x } _ { 1 : t } \\right)$, where $\\ell > 0$ is called the lag\n",
    "\n",
    "+ Prediction: to compute $p \\left( z _ { t + h } | \\mathbf { x } _ { 1 : t } \\right)$, where $h > 0$ is called prediction horizon.\n",
    "    $$p \\left( z _ { t + 2 } | \\mathbf { x } _ { 1 : t } \\right) = \\sum _ { z _ { t + 1 } } \\sum _ { z _ { t } } p \\left( z _ { t + 2 } | z _ { t + 1 } \\right) p \\left( z _ { t + 1 } | z _ { t } \\right) p \\left( z _ { t } | \\mathbf { x } _ { 1 : t } \\right)$$\n",
    "    Prediction about future observation: $$p \\left( \\mathbf { x } _ { t + h } | \\mathbf { x } _ { 1 : t } \\right) = \\sum _ { z _ { t + h } } p \\left( \\mathbf { x } _ { t + h } | z _ { t + h } \\right) p \\left( z _ { t + h } | \\mathbf { x } _ { 1 : t } \\right)$$\n",
    "    \n",
    "+ MAP estimation: to compute $\\arg \\max _ { \\mathbf { z } _ { 1 : T } } p \\left( \\mathbf { z } _ { 1 : T } | \\mathbf { x } _ { 1 : T } \\right)$, which is a most probable state sequence. ***Viterbi decoding*\n",
    "\n",
    "+ Posterior samples: $\\mathbf { z } _ { 1 : T } \\sim p \\left( \\mathbf { z } _ { 1 : T } | \\mathbf { x } _ { 1 : T } \\right)$\n",
    "\n",
    "+ Probability of the evidence: $p \\left( \\mathbf { X } _ { 1 : T } \\right)$ by summing up over all hidden paths, $p \\left( \\mathbf { x } _ { 1 : T } \\right) = \\sum _ { \\mathbf { z } _ { 1 : T } } p \\left( \\mathbf { z } _ { 1 : T } , \\mathbf { x } _ { 1 : T } \\right)$\n",
    "\n",
    "![](../images/17.HMM.png)\n",
    "\n",
    "### The forwards algorithm:\n",
    "Recursively compute the filtered marginals: $p \\left( z _ { t } | \\mathbf { x } _ { 1 : t } \\right)$\n",
    "\n",
    "We can also compute the log probability of the evidence:\n",
    "$$\\log p \\left( \\mathbf { x } _ { 1 : T } | \\boldsymbol { \\theta } \\right) = \\sum _ { t = 1 } ^ { T } \\log p \\left( \\mathbf { x } _ { t } | \\mathbf { x } _ { 1 : t - 1 } \\right) = \\sum _ { t = 1 } ^ { T } \\log Z _ { t }$$\n",
    "\n",
    "![](../images/17.Forward2.png)\n",
    "\n",
    "where $\\alpha_t = p \\left( z _ { t } | \\mathbf { x } _ { 1 : t } \\right)$ is the filtered belief state at time $t$ and is a vector of $K$ numbers. $Z _ { t } \\triangleq p \\left( \\mathbf { x } _ { t } | \\mathbf { x } _ { 1 : t - 1 } \\right)$ is normalization constant.\n",
    "\n",
    "### The forwards-backwards algorithm\n",
    "\n",
    "Compute the smoothed marginals: $p \\left( z _ { t } = j | \\mathbf { x } _ { 1 : T } \\right)$ \n",
    "\n",
    "Let: \n",
    "+ $\\alpha _ { t } ( j ) \\triangleq p \\left( z _ { t } = j | \\mathbf { x } _ { 1 : t } \\right)$ be the filered belief state\n",
    "+ $\\beta _ { t } ( j ) \\triangleq p \\left( \\mathbf { x } _ { t + 1 : T } | z _ { t } = j \\right)$ be the conditional likelihood of future evidence given that the hidden state at time $t$ is $j$. \n",
    "+ $\\psi_t(i,j) = B(i, j)$: Observation matrix\n",
    "+ $\\phi_t(i) = A(i)$: Transition matrix\n",
    "+ $\\gamma _ { t } ( j ) \\triangleq p \\left( z _ { t } = j | \\mathbf { x } _ { 1 : T } \\right)$ be the desired smoothed posterior marginals, \n",
    "\n",
    "    Since: $$p \\left( z _ { t } = j | \\mathbf { x } _ { 1 : T } \\right) \\propto p \\left( z _ { t } = j , \\mathbf { x } _ { t + 1 : T } | \\mathbf { x } _ { 1 : t } \\right) \\propto p \\left( z _ { t } = j | \\mathbf { x } _ { 1 : t } \\right) p \\left( \\mathbf { x } _ { t + 1 : T } | z _ { t } = j , \\mathbf { x } _ { 1 : t } \\right)$$\n",
    "\n",
    "    so we have: $$\\gamma _ { t } ( j ) \\propto \\alpha _ { t } ( j ) \\beta _ { t } ( j )$$\n",
    "\n",
    "    We compute $\\alpha$;s in a left-to-right fashion in forwards algorithm, we compute $\\beta$'s in right-to-left fashion. If we have already computed $\\beta_t$, we can compute $\\beta_{t-1}$ as follows:\n",
    "    $$\\boldsymbol { \\beta } _ { t - 1 } = \\boldsymbol { \\Phi } \\left( \\boldsymbol { \\psi } _ { t } \\odot \\boldsymbol { \\beta } _ { t } \\right)$$\n",
    "\n",
    "    with the base case: $$\\beta _ { T } ( i ) = p \\left( \\mathbf { x } _ { T + 1 : T } | z _ { T } = i \\right) = p ( \\emptyset | z _ { T } = i ) = 1$$ \n",
    "    which is the probability of a non-event.\n",
    "\n",
    "+ $\\xi _ { t , t + 1 } ( i , j ) \\triangleq  p \\left( z _ { t } = i , z _ { t + 1 } = j | \\mathbf { x } _ { 1 : T } \\right)$: be two-slice smooth marginals: \n",
    "    $$\\xi _ { t , t + 1 } ( i , j ) = \\alpha _ { t } ( i ) \\Phi _ { t + 1 } ( j ) \\beta _ { t + 1 } ( j ) \\psi ( i , j )$$\n",
    "    Or matrix form: $$\\boldsymbol { \\xi } _ { t , t + 1 } \\propto \\mathbf { \\Psi } \\odot \\left( \\boldsymbol { \\alpha } _ { t } \\left( \\boldsymbol { \\phi } _ { t + 1 } \\odot \\boldsymbol { \\beta } _ { t + 1 } \\right) ^ { T } \\right)$$\n",
    "    \n",
    "### The Viterbi algorithm: \n",
    "Compute: $\\mathbf { z } ^ { * } = \\arg \\max _ { \\mathbf { z } _ { 1 : T } } p \\left( \\mathbf { z } _ { 1 : T } | \\mathbf { x } _ { 1 : T } \\right)$\n",
    "\n",
    "Trellis diagram: the weight of the path $z_1, z_2, \\ldots, z_t$ given by:\n",
    "$$\\log \\pi _ { 1 } \\left( z _ { 1 } \\right) + \\log \\phi _ { 1 } \\left( z _ { 1 } \\right) + \\sum _ { t = 2 } ^ { T } \\left[ \\log \\psi \\left( z _ { t - 1 } , z _ { t } \\right) + \\log \\phi _ { t } \\left( z _ { t } \\right) \\right]$$\n",
    "\n",
    "![](../images/17.Viterbi.png)\n",
    "\n",
    "Let:\n",
    "+ $\\delta _ { t } ( j ) \\triangleq \\underset { z _ { 1 } , \\ldots , z _ { t - 1 } } { \\max } p \\left( \\mathbf { z } _ { 1 : t - 1 } , z _ { t } = j | \\mathbf { x } _ { 1 : t } \\right)$ be the proability of ending up in state $j$ at time $t$, given that we take the most probable path. The key insight is that the most probable path to state $j$ at time $t$ must consist of the most probable path to some other state $j$ at time $t-1$, followed by a transition from $i$ to $j$:\n",
    "\n",
    "    $$\\delta _ { t } ( j ) = \\max _ { i } \\delta _ { t - 1 } ( i ) \\psi ( i , j ) \\phi _ { t } ( j )$$\n",
    "    where $\\delta _ { 1 } ( j ) = \\pi _ { j } \\phi _ { 1 } ( j )$, initial state.\n",
    "    \n",
    "+ $\\alpha_t(j)$ tells us the most likeliy previous state on the most probable path to $z_t = j$.\n",
    "    $$a _ { t } ( j ) = \\underset { i } { \\operatorname { argmax } } \\delta _ { t - 1 } ( i ) \\psi ( i , j ) \\phi _ { t } ( j )$$\n",
    "    \n",
    "We compute the most probable final state: \n",
    "$z _ { T } ^ { * } = \\arg \\max _ { i } \\delta _ { T } ( i )$\n",
    "\n",
    "We compute the most probable sequence of states using traceback:\n",
    "$z _ { t } ^ { * } = a _ { t + 1 } \\left( z _ { t + 1 } ^ { * } \\right)$\n",
    "\n",
    "### Forwards filterings, backwards sampling:\n",
    "sample paths fromt he posterior:\n",
    "$$\\mathbf { z } _ { 1 : T } ^ { s } \\sim p \\left( \\mathbf { z } _ { 1 : T } | \\mathbf { x } _ { 1 : T } \\right)$$\n",
    "\n",
    "From posterior: \n",
    "$$p \\left( \\mathbf { z } _ { 1 : T } | \\mathbf { x } _ { 1 : T } \\right) = p \\left( z _ { T } | \\mathbf { x } _ { 1 : T } \\right) \\prod _ { t = T - 1 } ^ { 1 } p \\left( z _ { t } | z _ { t + 1 } , \\mathbf { x } _ { 1 : T } \\right)$$\n",
    "\n",
    "So we have the process: \n",
    "$$z _ { T } ^ { s } \\sim p \\left( z _ { T } = i | \\mathbf { x } _ { 1 : T } \\right) = \\alpha _ { T } ( i )$$\n",
    "\n",
    "then\n",
    "$$z_t \\sim p \\left( z _ { t } = i | z _ { t + 1 } = j , \\mathbf { x } _ { 1 : t } \\right) = \\frac { \\phi _ { t + 1 } ( j ) \\psi ( i , j ) \\alpha _ { t } ( i ) } { \\alpha _ { t + 1 } ( j ) }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
