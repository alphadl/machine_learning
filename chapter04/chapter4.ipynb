{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Gaussian models (MVN - Multivariate Normal)\n",
    "\n",
    "## 1. Single Gaussian\n",
    "$$\\mathcal { N } ( \\mathbf { x } | \\boldsymbol { \\mu } , \\mathbf { \\Sigma } ) \\triangleq \\frac { 1 } { ( 2 \\pi ) ^ { D / 2 } | \\mathbf { \\Sigma } | ^ { 1 / 2 } } \\exp \\left[ - \\frac { 1 } { 2 } ( \\mathbf { x } - \\boldsymbol { \\mu } ) ^ { T } \\mathbf { \\Sigma } ^ { - 1 } ( \\mathbf { x } - \\boldsymbol { \\mu } ) \\right]$$\n",
    "\n",
    "eigendecomposition of $\\Sigma$: $\\boldsymbol { \\Sigma } = \\mathbf { U } \\mathbf { \\Lambda } \\mathbf { U } ^ { T }$\n",
    "$$\\mathbf { \\Sigma } ^ { - 1 } = \\mathbf { U } ^ { - T } \\mathbf { \\Lambda } ^ { - 1 } \\mathbf { U } ^ { - 1 } = \\mathbf { U } \\mathbf { \\Lambda } ^ { - 1 } \\mathbf { U } ^ { T } = \\sum _ { i = 1 } ^ { D } \\frac { 1 } { \\lambda _ { i } } \\mathbf { u } _ { i } \\mathbf { u } _ { i } ^ { T }$$\n",
    "\n",
    "$$\\begin{aligned} ( \\mathbf { x } - \\boldsymbol { \\mu } ) ^ { T } \\mathbf { \\Sigma } ^ { - 1 } ( \\mathbf { x } - \\boldsymbol { \\mu } ) & = ( \\mathbf { x } - \\boldsymbol { \\mu } ) ^ { T } \\left( \\sum _ { i = 1 } ^ { D } \\frac { 1 } { \\lambda _ { i } } \\mathbf { u } _ { i } \\mathbf { u } _ { i } ^ { T } \\right) ( \\mathbf { x } - \\boldsymbol { \\mu } ) \\\\ & = \\sum _ { i = 1 } ^ { D } \\frac { 1 } { \\lambda _ { i } } ( \\mathbf { x } - \\boldsymbol { \\mu } ) ^ { T } \\mathbf { u } _ { i } \\mathbf { u } _ { i } ^ { T } ( \\mathbf { x } - \\boldsymbol { \\mu } ) = \\sum _ { i = 1 } ^ { D } \\frac { y _ { i } ^ { 2 } } { \\lambda _ { i } } \\end{aligned} $$ \n",
    "$$\\text { where } y _ { i } \\triangleq \\mathbf { u } _ { i } ^ { T } ( \\mathbf { x } - \\boldsymbol { \\mu } )$$\n",
    "\n",
    "Mahalanobis distance corresponds to Euclidean distance in a transformed coordinate system, where we shift by $\\mu$ and rotate by $U$\n",
    "\n",
    "### MLE for an MVN\n",
    "If we have N iid samples $x_i âˆ¼ N(\\mu, \\Sigma)$, then the MLE for the parameters is given by\n",
    "\\begin{aligned} \\hat { \\boldsymbol { \\mu } } _ { m l e } & = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\mathbf { x } _ { i } \\triangleq \\overline { \\mathbf { x } } \\\\ \\hat { \\mathbf { \\Sigma } } _ { m l e } & = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\left( \\mathbf { x } _ { i } - \\overline { \\mathbf { x } } \\right) \\left( \\mathbf { x } _ { i } - \\overline { \\mathbf { x } } \\right) ^ { T } = \\frac { 1 } { N } \\left( \\sum _ { i = 1 } ^ { N } \\mathbf { x } _ { i } \\mathbf { x } _ { i } ^ { T } \\right) - \\overline { \\mathbf { x } } \\overline { \\mathbf { x } } ^ { T } \\end{aligned}\n",
    "\n",
    "log-likelihood:\n",
    "$$\\ell ( \\boldsymbol { \\mu } , \\boldsymbol { \\Sigma } ) = \\log p ( \\mathcal { D } | \\boldsymbol { \\mu } , \\mathbf { \\Sigma } ) = \\frac { N } { 2 } \\log | \\boldsymbol { \\Lambda } | - \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { N } \\left( \\mathbf { x } _ { i } - \\boldsymbol { \\mu } \\right) ^ { T } \\mathbf { \\Lambda } \\left( \\mathbf { x } _ { i } - \\boldsymbol { \\mu } \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian Discriminant Analysis\n",
    "\n",
    "Class conditional density (likelihood): \n",
    "$$p ( \\mathbf { x } | y = c , \\boldsymbol { \\theta } ) = \\mathcal { N } ( \\mathbf { x } | \\boldsymbol { \\mu } _ { c } , \\mathbf { \\Sigma } _ { c } )$$\n",
    "\n",
    "Posterior over class labels:\n",
    "\n",
    "### Quadratic Discriminant Analysis (QDA)\n",
    "$$p ( y = c | \\mathbf { x } , \\boldsymbol { \\theta } ) = \\frac { p ( y = c | \\boldsymbol { \\theta } ) p ( \\mathbf { x } | y = c , \\boldsymbol { \\theta } ) } { \\sum _ { c ^ { \\prime } } p \\left( y = c ^ { \\prime } | \\boldsymbol { \\theta } \\right) p ( \\mathbf { x } | y = c ^ { \\prime } , \\boldsymbol { \\theta } ) } = \\frac { \\pi _ { c } \\left| 2 \\pi \\boldsymbol { \\Sigma } _ { c } \\right| ^ { - \\frac { 1 } { 2 } } \\exp \\left[ - \\frac { 1 } { 2 } \\left( \\mathbf { x } - \\boldsymbol { \\mu } _ { c } \\right) ^ { T } \\boldsymbol { \\Sigma } _ { c } ^ { - 1 } \\left( \\mathbf { x } - \\boldsymbol { \\mu } _ { c } \\right) \\right] } { \\sum _ { c ^ { \\prime } } \\pi _ { c ^ { \\prime } } \\left| 2 \\pi \\boldsymbol { \\Sigma } _ { c ^ { \\prime } } \\right| ^ { - \\frac { 1 } { 2 } } \\exp \\left[ - \\frac { 1 } { 2 } \\left( \\mathbf { x } - \\boldsymbol { \\mu } _ { c ^ { \\prime } } \\right) ^ { T } \\mathbf { \\Sigma } _ { c ^ { \\prime } } ^ { - 1 } \\left( \\mathbf { x } - \\boldsymbol { \\mu } _ { c ^ { \\prime } } \\right) \\right] }$$\n",
    "\n",
    "![QDA visualization](../images/3.QDA.png)\n",
    "\n",
    "\n",
    "### Linear Discriminant Analysis (LDA) \n",
    "(special case of QDA where $\\Sigma_c = \\Sigma$ shared across classes)\n",
    "$$\\begin{aligned} p ( y = c | \\mathbf { x } , \\boldsymbol { \\theta } ) & \\propto \\pi _ { c } \\exp \\left[ \\boldsymbol { \\mu } _ { c } ^ { T } \\mathbf { \\Sigma } ^ { - 1 } \\mathbf { x } - \\frac { 1 } { 2 } \\mathbf { x } ^ { T } \\mathbf { \\Sigma } ^ { - 1 } \\mathbf { x } - \\frac { 1 } { 2 } \\boldsymbol { \\mu } _ { c } ^ { T } \\mathbf { \\Sigma } ^ { - 1 } \\boldsymbol { \\mu } _ { c } \\right] \\\\ & = \\exp \\left[ \\boldsymbol { \\mu } _ { c } ^ { T } \\mathbf { \\Sigma } ^ { - 1 } \\mathbf { x } - \\frac { 1 } { 2 } \\boldsymbol { \\mu } _ { c } ^ { T } \\mathbf { \\Sigma } ^ { - 1 } \\boldsymbol { \\mu } _ { c } + \\log \\pi _ { c } \\right] \\exp \\left[ - \\frac { 1 } { 2 } \\mathbf { x } ^ { T } \\mathbf { \\Sigma } ^ { - 1 } \\mathbf { x } \\right] \\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned} \\gamma _ { c } & = - \\frac { 1 } { 2 } \\boldsymbol { \\mu } _ { c } ^ { T } \\boldsymbol { \\Sigma } ^ { - 1 } \\boldsymbol { \\mu } _ { c } + \\log \\pi _ { c } \\\\ \\boldsymbol { \\beta } _ { c } & = \\boldsymbol { \\Sigma } ^ { - 1 } \\boldsymbol { \\mu } _ { c } \\end{aligned}$$\n",
    "\n",
    "$$p ( y = c | \\mathbf { x } , \\boldsymbol { \\theta } ) = \\frac { e ^ { \\boldsymbol { \\beta } _ { c } ^ { T } \\mathbf { x } + \\gamma _ { c } } } { \\sum _ { c ^ { \\prime } } e ^ { \\boldsymbol { \\beta } _ { c ^ { \\prime } } ^ { T } \\mathbf { x } + \\gamma _ { c ^ { \\prime } } } } = \\mathcal { S } ( \\boldsymbol { \\eta } ) _ { c }$$\n",
    "\n",
    "![LDA visualization](../images/3.LDA.png)\n",
    "\n",
    "### MLE for Discriminant Analysis\n",
    "log-likelihood function:\n",
    "$$\\log p ( \\mathcal { D } | \\boldsymbol { \\theta } ) = \\left[ \\sum _ { i = 1 } ^ { N } \\sum _ { c = 1 } ^ { C } \\mathbb { I } \\left( y _ { i } = c \\right) \\log \\pi _ { c } \\right] + \\sum _ { c = 1 } ^ { C } \\left[ \\sum _ { i : y _ { i } = c } \\log \\mathcal { N } ( \\mathbf { x } | \\boldsymbol { \\mu } _ { c } , \\boldsymbol { \\Sigma } _ { c } ) \\right]$$\n",
    "\n",
    "Take derivative we get:\n",
    "$$\\hat { \\pi } _ { c } = \\frac { N _ { c } } { N }$$\n",
    "$$\\hat { \\boldsymbol { \\mu } } _ { c } = \\frac { 1 } { N _ { c } } \\sum _ { i : y _ { i } = c } \\mathbf { x } _ { i } , \\quad \\hat { \\mathbf { \\Sigma } } _ { c } = \\frac { 1 } { N _ { c } } \\sum _ { i : y _ { i } = c } \\left( \\mathbf { x } _ { i } - \\hat { \\boldsymbol { \\mu } } _ { c } \\right) \\left( \\mathbf { x } _ { i } - \\hat { \\boldsymbol { \\mu } } _ { c } \\right) ^ { T }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
